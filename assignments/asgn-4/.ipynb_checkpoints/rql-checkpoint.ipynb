{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f662d82a",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#333333; text-align:center; line-height: 0;\"> <img style=\"right;\" src=\"logo.png\" width=18% height=18%> Reinforcement Learning | Assignment 4 \n",
    "</h1>\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "The goal of this assignment is to implement:\n",
    "\n",
    "- Actor\n",
    "- Critic model\n",
    "\n",
    "\n",
    "___Total points:___ 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de656c3a",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\"> Section 1: Introduction to model predictive control (MPC) </h2>\n",
    "\n",
    "***\n",
    "###  <font color=\"blue\"> 1.1 Intuition behind MPC </font>\n",
    "**MPC** is one of the most popular methods of obtaining the optimal controller and, in fact, is an industry standard.\n",
    "Let us start from the most reasonable question. Why should we care about **MPC**? Why do we even need that thing? Are there any problems that naturally lead to the development of such a model?\n",
    "\n",
    "In fact, yes. There are infinitely many applications of **MPC**, including petroleum extraction, agricultural facilities, automatic assembly and so on. But let us take a look at the development of such a type of machines that are impressive not only to an expert in the field - humanoid robots. If you've ever seen a video of Boston Dynamics' humanoid robots, you've probably wondered, how do the engineers made it work? How does such a complex machine make those precise movements so that it is capable of performing acrobatics that already exceeds the abilities of a typical human in some aspects (i.e. backflip)?\n",
    "\n",
    "The answer is, you guessed it, **MPC**. \n",
    "\n",
    "The performance of the modern robots comes with its cost. The problem of generating control for any robot requires:\n",
    "\n",
    "- real time performance, as there are in general almost no stable states in the robot's movement, and all the calculations should be performed quickly\n",
    "- optimizing a complex composite cost to a certain time horizon in order to follow the high-level plan\n",
    "- (in some cases) discrete-continuous optimization, which is difficult\n",
    "- taking into account various types of constraints, i.e.\n",
    "    1. torque and angle limits for the servomotors (in a form of inequality)\n",
    "    2. functional constraints following from the problem statement, that do not always allow for an analytical solution \n",
    "    3. also there could be constraints on foot placement, body placement, slippery surfaces, etc.\n",
    "    4. constraints of the limbs non-intersection (if this is a case)\n",
    "\n",
    "All in all, such a problem could lack a closed form solution, like $\\boldsymbol u = \\boldsymbol f(\\text{state}, \\text{target})$.\n",
    "\n",
    "One of the most fundamental important ways to obtain an optimal controller is __MPC__. \n",
    "\n",
    "* __MPC-like algorithms__ are ones of the few that can handle very complex constraints, including functional, nonlinear, nonconvex, etc.\n",
    "\n",
    "Informally speaking, algorithms that generate control for a complex dynamical system typically have to be predictive. And taking the system model into account helps along the way. In order to optimize an objective along the trajectory, we could try to estimate the system behaviour in the future. This is litearlly what **MPC** does. With that in hand, let us state the problem and the **MPC** more formally.\n",
    "\n",
    "###  <font color=\"blue\"> 1.2 MPC mathematical description </font>\n",
    "<a id='2.2'></a>\n",
    "\n",
    "First of all, we consider a controlled physical system described by the system of ordinary differential equations \n",
    "<a id='System'></a>\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\dot{\\boldsymbol x} = \\boldsymbol f(\\boldsymbol x, \\boldsymbol u)\\\\\n",
    "\\boldsymbol y = \\boldsymbol h(\\boldsymbol x) \\\\\n",
    "\\boldsymbol x(0)=\\boldsymbol x_{0}\\\\\n",
    "\\end{cases}\n",
    "\\end{equation} where $x_{0}$ is the **initial state**.In the following table we introduce some basic notation. From now on, we will write vectors in **bold**.\n",
    "<a id='Notation'></a>\n",
    "\n",
    "| Notation &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;&nbsp;Description |\n",
    "|:-----------------------:|-------------|\n",
    "| $\\boldsymbol f(\\cdot, \\cdot, \\cdot) : \\mathbb{R}^{n+1}\\times \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{n}$ |A **state dynamic function** or, more informally, **righ-hand-side** of a system <br /> of ordinary differential equations $\\dot{\\boldsymbol x} = \\boldsymbol f(t, \\boldsymbol x, \\boldsymbol u)$|\n",
    "| $\\boldsymbol x \\in \\mathbb{R}^{n} $ | An element of the **state space** of a controlled system of dimensionality $n$ |\n",
    "| $\\boldsymbol u \\in \\mathbb{R}^{m}$ | An element of the **action space** of a controlled system of dimensionality $m$ |\n",
    "| $\\boldsymbol y \\in \\mathbb{R}^{k}$ | An **observartion**|\n",
    "| $\\mathbb{X}\\subset \\mathbb{R}^{n} $| **State constraint set**|\n",
    "| $\\mathbb{U}\\subset \\mathbb{R}^{m} $| **Action constraint set**|\n",
    "| $\\boldsymbol h(\\cdot): \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{k}$ | **Observation function**  |\n",
    "| $\\boldsymbol\\rho(\\cdot) : \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ | **Policy** function |\n",
    "| $r(\\cdot) : \\mathbb{R}^n \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}$ | **Running cost** function  |\n",
    "\n",
    "\n",
    "Here we would like to make a few clarifications on the notation introduced.\n",
    "* For **MPC** N-step cost has the following form  $J_N(\\boldsymbol x_{k}, \\kappa(\\cdot)):=\\int_{k\\delta}^{(k+N)\\delta}\\rho(\\boldsymbol x(t), \\kappa(\\boldsymbol x(t))) dt$\n",
    "* In the following problems we consider $\\mathbb{X} = \\mathbb{R}^{n}$ without any remarks but in general we might want to introduce some reasonable **state constraint set**.\n",
    "* When working with the system, we cannot know the exact value of the characteristics we are interested in. What we do is **make a measurement** calculatung **observation function** $\\boldsymbol h(\\cdot)$. The result we call an **observation** $\\boldsymbol y$. For now we consider $\\boldsymbol y = \\boldsymbol h(\\boldsymbol x) := \\boldsymbol x$ further but in general it might not be so! For example, if we control a body motion on a plane $(x,y)$, we could measure a distance to the origin: $\\boldsymbol h(\\boldsymbol x) = \\lVert \\boldsymbol x \\rVert$. \n",
    "* Most modern controllers are digital, therefore the control signal is generated by sampling with some **sampling time** $\\delta$. In that case we call it **digital control setting**. The following table gives a comparison between mathematical description of the original setting of the controlled system and its **digital control setting**.\n",
    "<a id='Comparison_table'></a>\n",
    "|  Original setting | Digital control setting |\n",
    "|:-----------------------|:--------------------------|\n",
    "|1) $ \\qquad \\dot{\\boldsymbol x}=\\boldsymbol f\\left(\\boldsymbol x, \\boldsymbol u\\right) $ | $\\text{1*)} \\quad \\dot{\\boldsymbol x}=\\boldsymbol f\\left(\\boldsymbol x, \\boldsymbol u^{\\delta}\\right)$|\n",
    "| $\\text{2)} \\quad \\boldsymbol u=\\boldsymbol u(t), t \\in[0, T]$  | $\\text{2*)} \\quad \\boldsymbol u_k(t)=\\boldsymbol u(k\\delta) , t \\in[k \\delta,(k+1) \\delta]$  | \n",
    "| $$\\text{3)} \\quad \\boldsymbol x(t):=\\boldsymbol x_{t_0}+\\int_{t_0}^{t}f\\left(\\boldsymbol x(\\tau), \\boldsymbol u(\\tau) \\right) d \\tau$$| $$\\text{3*)} \\quad \\boldsymbol x^{\\boldsymbol u_{k}}(t):=\\boldsymbol x_{k}+\\int_{k \\delta}^{t}f\\left(\\boldsymbol x(\\tau), \\boldsymbol u_{k}\\right) d \\tau$$|     \n",
    "\n",
    "<!--- $$\\boldsymbol x_{i \\mid k}:=\\boldsymbol x((k+i-1) \\delta), k \\in \\mathbb{N}$$\n",
    "$\\boldsymbol u^{\\delta}(t) \\equiv \\boldsymbol u_{i \\mid k}=\\kappa\\left(\\boldsymbol x_{i \\mid k}\\right), t \\in[k \\delta,(k+i) \\delta]$ \n",
    "One can see that for any $k \\in \\mathbb{N}$, the state $x^{u_{k}}(t)$ at $t \\geq k \\delta$ under $u_{k}$ satisfies\n",
    "-->\n",
    "<p style=\"text-align: center;\">\n",
    "\n",
    "</p>\n",
    "\n",
    "From now and on by the system we mean a system in the **digital control setting**, which is illustrated on the figure below .\n",
    "<img src=\"digital_control_setting.svg\" width=40% height=40% />\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "In general, our optimal control problem is written as $\\min_{\\kappa(\\cdot)}\\int_{k\\delta}^{(k+N)\\delta}\\rho(\\boldsymbol x(t), \\kappa(\\boldsymbol x(t))) dt$, where $N$ is **prediction horizon**. But in **MPC** we switch to a discrete sum instead of an integral: $$\\min _{\\boldsymbol u_{k + i}: i=0, \\ldots, N-1} \\left(\\sum_{i=1}^{N-1} \\rho \\left(\\hat{\\boldsymbol x}_{k + i}, \\boldsymbol u_{k + i}\\right)\\delta \\right) $$ Wich is obviously equivalent to the following form:\n",
    "$$\\min _{ \\boldsymbol u_{k + i}: i=0, \\ldots, N-1} \\left(\\sum_{i=1}^{N-1} \\rho \\left(\\hat{\\boldsymbol x}_{k + i}, \\boldsymbol u_{k + i}\\right)\\right)$$  \n",
    "And now we see that it fits conviniently in our **digital control setting**: we will just find a minimizing sequence of actions for our digital model predictive control for stage costs predicted for N steps forward.  Notice that under digital control our state evolves according to [3*)](#Comparison_table) . Now we need to somehow numerically evaluate the interal $\\int_{k \\delta}^{t}f\\left(\\boldsymbol x(\\tau), \\boldsymbol u_{k}\\right) d \\tau$. In order to do this one might use any numerical integration scheme. For example the **Euler scheme** :\n",
    "<a id='Euler'></a>\n",
    "$$\n",
    "\\boldsymbol x_{k+1}=\\boldsymbol x_{k}+\\delta \\boldsymbol f\\left(\\boldsymbol x_{k}, \\boldsymbol u_{k}\\right) \\text {, }\n",
    "$$\n",
    "#### Algorithm <sup>[1]</sup>:\n",
    "<a id='Objective'></a>\n",
    "Let us describe the **MPC** algorithm for the problem just described above.  \n",
    "At the current state $\\boldsymbol x_{k}$ :\n",
    "\n",
    "(a) **MPC** solves an $N$-step lookahead  problem: $\\min _{\\boldsymbol u_{k + i}: i=0, \\ldots, N-1} \\left(\\sum_{i=0}^{N-1} \\rho \\left(\\hat{\\boldsymbol x}_{k + i}, \\boldsymbol u_{k + i}\\right)\\right)$\n",
    "\n",
    "(b) If $\\left\\{\\boldsymbol u^{*}_{k}, \\ldots, \\boldsymbol u^{*}_{k+N-1}\\right\\}$ is the optimal control sequence of this problem, **MPC** applies $\\boldsymbol u^{*}_{k}$ and discards the other controls $\\boldsymbol u^{*}_{k+1}, \\ldots, \\boldsymbol u^{*}_{k+N-1}$. \n",
    "\n",
    "(c) At the next stage, **MPC** repeats this process, once the next state $\\boldsymbol x_{k}$ is revealed.\n",
    "\n",
    "\n",
    "<img src=\"MPC.svg\" width=35% height=35% />\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df53579",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\"\"\"\n",
    "Just importing all the necessary stuff here.\n",
    "DO NOT CHANGE\n",
    "\"\"\"\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from rcognita_framework.pipelines.pipeline_3wrobot_NI import Pipeline3WRobotNI\n",
    "from rcognita_framework.rcognita.actors import Actor\n",
    "from rcognita_framework.rcognita.critics import CriticActionValue\n",
    "from rcognita_framework.rcognita.models import ModelNN, ModelWeightContainer\n",
    "from rcognita_framework.rcognita.scenarios import EpisodicScenarioBase\n",
    "from rcognita_framework.rcognita import models\n",
    "from rcognita_framework.rcognita.utilities import rc\n",
    "from rcognita_framework.rcognita.optimizers import SciPyOptimizer, TorchOptimizer\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50f3c10",
   "metadata": {},
   "source": [
    "In the following, for testing, we will use a system representing a three-wheeled robot described by a system of equations\n",
    "$$\\begin{cases}\n",
    "\\dot{x}_c=v \\cos \\alpha \\\\\n",
    "\\dot{y}_c=v \\sin \\alpha \\\\\n",
    "\\dot{\\alpha}=\\omega\n",
    "\\end{cases}$$ where $x_c$ and $y_c$ are coordinates of the center of mass, $v$ and $\\omega$ are velocity of the center of mass and angular velocity respectively and these are components of the control $\\boldsymbol u := (v, \\omega)$ as well.  \n",
    "The final visualization will look like the following picture:\n",
    "<img src=\"robot.png\" width=60% height=60% />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b700ed",
   "metadata": {},
   "source": [
    "## Main goal:\n",
    "Main goal here is to impement an RQL algorithm to combine both just described model-based and model-free approaches.\n",
    "\n",
    "<h2 style=\"color:#A7BD3F;\"> Section 1: Actor RQL implementation </h2>\n",
    "\n",
    "Rollout Q-learning (RQL) actor optimizes the following actor objective:\n",
    "            $J^a \\left( y_k| \\{u\\}_k^{N_a} \\right) = \\sum_{i=0}^{N_a-1} \\gamma^i r(y_{k+i}, u_{k+i}) + \\gamma^{N_a} Q(y_{k+N_a}, u_{k+N_a}) = \\text{MPC_objective} + \\gamma^{N_a}\\cdot\\text{critic}$\n",
    "            \n",
    "Implement this objective in the `objective` method of `Actor`\n",
    "            \n",
    "**A brief recall**:\n",
    "\n",
    "* `self.discount_factor` - that's how you can obtain $\\gamma$ inside critic and actor\n",
    "* `self.running_objective(observation, action)` - that's how you can obtain a value $r(\\boldsymbol x, \\boldsymbol u)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorRQLStudent(Actor):\n",
    "    \n",
    "    def objective(\n",
    "        self, action_sequence, observation,\n",
    "    ):\n",
    "\n",
    "        action_sequence_reshaped = rc.reshape(\n",
    "            action_sequence, [self.prediction_horizon + 1, self.dim_input]\n",
    "        ) ## use this as action sequence\n",
    "\n",
    "        observation_sequence = [observation]\n",
    "\n",
    "        observation_sequence_predicted = self.predictor.predict_sequence(\n",
    "            observation, action_sequence_reshaped\n",
    "        )\n",
    "\n",
    "        observation_sequence = rc.vstack(\n",
    "            (\n",
    "                rc.reshape(observation, [1, self.dim_output]),\n",
    "                observation_sequence_predicted,\n",
    "            )\n",
    "        ) ## use this as observation sequence\n",
    "\n",
    "        actor_objective = 0\n",
    "        \n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "\n",
    "        \n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################\n",
    "        return actor_objective\n",
    "    \n",
    "    def update(self, observation, constraint_functions=(), time=None):\n",
    "        \"\"\"\n",
    "        Method to update the current action or weight tensor.\n",
    "        The old (previous) action or weight tensor is stored.\n",
    "        The `time` argument is used for debugging purposes.\n",
    "        \"\"\"\n",
    "        constraints = ()\n",
    "        action_sequence_old = rc.rep_mat(\n",
    "            self.action_old, 1, self.prediction_horizon + 1\n",
    "        )\n",
    "\n",
    "        action_sequence_init_reshaped = rc.reshape(\n",
    "            action_sequence_old, [(self.prediction_horizon + 1) * self.dim_input,],\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        actor_objective = rc.func_to_lambda_with_params(\n",
    "            self.objective, observation, var_prototype=action_sequence_init_reshaped\n",
    "        )\n",
    "\n",
    "\n",
    "        action_sequence_optimized = self.optimizer.optimize(\n",
    "            actor_objective,\n",
    "            action_sequence_init_reshaped,\n",
    "            self.action_bounds,\n",
    "            constraints=constraints,\n",
    "        )\n",
    "\n",
    "        self.model.update_and_cache_weights(action_sequence_optimized[: self.dim_input])\n",
    "        self.action_old = self.model.cache.weights\n",
    "        self.action = self.model.weights\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddf2973",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\"> Section 2: Critic model implementation </h2>\n",
    "\n",
    "This task technically doesn't somehow differ from the corresponding task from the previous assignment. Choose sign-definiteness of your critic wisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelNNStudent(ModelNN):\n",
    "\n",
    "    model_name = \"NN\"\n",
    "\n",
    "    def __init__(self, dim_observation, dim_action, *args, weights = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "\n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################\n",
    "        \n",
    "        if weights is not None:\n",
    "            self.load_state_dict(weights)\n",
    "        self.double()\n",
    "        self.cache_weights()\n",
    "\n",
    "    def forward(self, input_tensor, weights=None):\n",
    "        if weights is not None:\n",
    "            self.update(weights)\n",
    "        \n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "        \n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d11d41c",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\"> Section 3: Testing</h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b3b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline3WRobotNIStudent(Pipeline3WRobotNI):\n",
    "\n",
    "    def initialize_models(self):\n",
    "        self.actor_model = models.ModelWeightContainer(weights_init=self.action_init)\n",
    "        self.critic_model = ModelNNStudent(self.dim_output, self.dim_input)\n",
    "        self.model_running_objective = models.ModelQuadForm(weights=self.R1)\n",
    "\n",
    "        \n",
    "    def initialize_optimizers(self):\n",
    "\n",
    "        opt_options_torch = {\"lr\": self.critic_learning_rate}\n",
    "        opt_options_scipy = {\n",
    "            \"maxiter\": 1250,\n",
    "            \"maxfev\": 5000,\n",
    "            \"disp\": False,\n",
    "            \"adaptive\": True,\n",
    "            \"xatol\": 1e-7,\n",
    "            \"fatol\": 1e-7,\n",
    "        }\n",
    "    \n",
    "        self.actor_optimizer = SciPyOptimizer(\n",
    "            opt_method=\"SLSQP\", opt_options=opt_options_scipy\n",
    "        )\n",
    "        self.critic_optimizer = TorchOptimizer(opt_options_torch)\n",
    "\n",
    "    def initialize_actor_critic(self):\n",
    "        self.critic = CriticActionValue(\n",
    "            dim_input=self.dim_input,\n",
    "            dim_output=self.dim_output,\n",
    "            data_buffer_size=self.data_buffer_size,\n",
    "            running_objective=self.running_objective,\n",
    "            discount_factor=self.discount_factor,\n",
    "            optimizer=self.critic_optimizer,\n",
    "            model=self.critic_model,\n",
    "            sampling_time=self.sampling_time,\n",
    "        )\n",
    "        self.actor = ActorRQLStudent(\n",
    "            self.prediction_horizon,\n",
    "            self.dim_input,\n",
    "            self.dim_output,\n",
    "            self.control_mode,\n",
    "            self.action_bounds,\n",
    "            action_init=self.action_init,\n",
    "            predictor=self.predictor,\n",
    "            optimizer=self.actor_optimizer,\n",
    "            critic=self.critic,\n",
    "            running_objective=self.running_objective,\n",
    "            model=self.actor_model,\n",
    "        )\n",
    "        \n",
    "    def initialize_scenario(self):\n",
    "        self.scenario = EpisodicScenarioBase(\n",
    "            system=self.system,\n",
    "            simulator=self.simulator,\n",
    "            controller=self.controller,\n",
    "            actor=self.actor,\n",
    "            critic=self.critic,\n",
    "            logger=self.logger,\n",
    "            datafiles=self.datafiles,\n",
    "            time_final=self.time_final,\n",
    "            running_objective=self.running_objective,\n",
    "            no_print=self.no_print,\n",
    "            is_log=self.is_log,\n",
    "            is_playback=self.is_playback,\n",
    "            N_episodes=self.N_episodes,\n",
    "            N_iterations=self.N_iterations,\n",
    "            state_init=self.state_init,\n",
    "            action_init=self.action_init,\n",
    "        )\n",
    "\n",
    "    def execute_pipeline(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Full execution routine\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        super().execute_pipeline(**kwargs)\n",
    "    \n",
    "##### Execution here!!! Full list of kwargs can be seen at \n",
    "##### rcognita_framework.pipelines.config_blueprints in the ConfigInvertedPendulum\n",
    "\n",
    "########### Play around here, tune lr and then go to the grading part.\n",
    "pipeline = Pipeline3WRobotNIStudent()\n",
    "pipeline.execute_pipeline(\n",
    "    no_visual=True, \n",
    "    is_playback=False, # Don't touch\n",
    "    critic_learning_rate= ,# Set your lr here\n",
    "    no_print=False,\n",
    "    time_final=10, \n",
    "    N_episodes=1, \n",
    "    N_iterations=1, \n",
    "    prediction_horizon=3,\n",
    "    sampling_time=0.02, # Do not change it!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86a72d5",
   "metadata": {},
   "source": [
    "#### Grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7354d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline3WRobotNIStudent()\n",
    "pipeline.execute_pipeline(\n",
    "    no_visual=True, \n",
    "    is_playback=False, \n",
    "    critic_learning_rate= ,# Set your lr here\n",
    "    no_print=True,\n",
    "    time_final=10, # Don't touch\n",
    "    N_episodes=1, \n",
    "    N_iterations=1, \n",
    "    prediction_horizon=3,\n",
    "    sampling_time=0.02, # Do not change it!\n",
    ")\n",
    "import numpy as np\n",
    "trajectory = np.array(pipeline.scenario.trajectory)\n",
    "grade = np.clip(-1.5 * np.sum(trajectory[2500:, :3]) + 303, 0, 100)\n",
    "print(f\"Your final grade is {grade}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e724e156",
   "metadata": {},
   "source": [
    "### A set of questions. Leave your answers in this cell.\n",
    "##### Set your prediction_horizon in the playground cell very small (mb even nulify it) and launch. What do you see? How do you explain it?\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c2fdd1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06889641",
   "metadata": {},
   "source": [
    "##### Set your prediction_horizon bigger than 5 in the playground cell and launch . What do you see? How do you explain it?\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9c8180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f591ba52",
   "metadata": {},
   "source": [
    "##### Set sampling_time to 0.1 in the playground cell and launch. What do you see? How do you explain it? Try to experiment with prediction_horizon now.\n",
    "##### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08b40cb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
