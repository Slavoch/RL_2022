{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78950ef2",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#333333; text-align:center; line-height: 0;\"> <img style=\"right;\" src=\"logo.png\" width=18% height=18%> Reinforcement Learning | Assignment 1 \n",
    "</h1>\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "The goal of this assignment is to implement:\n",
    "- value iteration algorithm (35 points)\n",
    "- policy iteration algorithm (35 points)\n",
    "- Q-learning algorithm (30 points)\n",
    "\n",
    "___Total points:___ 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35862c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\"\"\"\n",
    "launch me before you start\n",
    "\"\"\"\n",
    "from rcognita_framework.pipelines.pipeline_grid_world import action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2558a7",
   "metadata": {},
   "source": [
    "###  <font color=\"blue\"> A brief introduction </font>\n",
    "Examine it carefully, it covers most of your possible needs to make an assignment.\n",
    "\n",
    "***\n",
    "\n",
    "### About Rcognita\n",
    "The platform for this (and all subsequent work) is [Rcognita](https://gitflic.ru/project/aidynamicaction/rcognita), a framework for applying control theory and machine learning algorithms to control problems, an integral part of which is the closed-loop interaction between the agent under control and the environment evolving over time. In the Rcognita paradigm, the main bearer of all the classes and variables needed to run the simulation is the `pipeline`. \n",
    "\n",
    "The main parts of `pipeline` are: \n",
    "* `simulator`, which is defined at module `simulators.py` and responsible for simulation of evolution of the environment\n",
    "* `actor`, defined at module `actors.py`, which is responsible for obtaining of action\n",
    "* `critic`, defined at module `critics.py`, which is reponsible for learning of reward function and obtaining its value \n",
    "* `controller`, which is defined at module `controllers.py` and it's needed to put it all together into an RL (or other) controller\n",
    "* `system`, which is defined at module `systems.py`. (Possible variations are ODE or discrete, like our grid world)\n",
    "\n",
    "Other minor things are also declarated in the pipeline and assembled module by module up to the execution of the pipeline itself. \n",
    "Just to be on the same page, we provide some notation to prevent further confusions.\n",
    "* `weights` is the general name and for weights of neural network and for values in tables of value function and policy as well. This agreement comes from the motivation for being consistent with classical RL where critic and actor are being implemented as some neural networks with some **weights**. So, here comes the second term\n",
    "* `model`. It's obvious that parameters give specificity to something. But the general form itself is being called `model`. There are plenty of models of different types and forms (such as NN). Model is what critic and actor and even running cost always have, no matter what.\n",
    "* `predictor` - Inspite of it's cryptic name, this object performs an important function, namely, it carries the law by which the dynamics of our system is being predicted in future. For example, if we have some differential equation\n",
    "$\n",
    "\\begin{cases}\n",
    "\\dot{\\boldsymbol x} = \\boldsymbol f(\\boldsymbol x, \\boldsymbol u)\\\\\n",
    "\\boldsymbol y = h(\\boldsymbol x) \\\\\n",
    "\\boldsymbol x(0)=\\boldsymbol x_{0}\\\\\n",
    "\\end{cases}\n",
    "$\n",
    "in general, there are several ways of prediction: \n",
    "> - **Analytical**, when we have a precise formula of analytical solution $\\boldsymbol x(t)$ to the ODE and have no problems to compute it at any given time. This is great but not that possible in real life. Nevertheless, our predictor could be expressed like:  $\\text{predictor}(\\boldsymbol x(\\tau),dt) = \\boldsymbol x(\\tau + dt)$\n",
    "> - **Numerical** way is mostly a case. The simplest way of prediction then is an Euler method:\n",
    "$\\boldsymbol x_{k+1}= \\text{predictor}(\\boldsymbol x_k, \\delta)=\\boldsymbol x_{k}+\\delta \\boldsymbol f\\left(\\boldsymbol x_{k}, \\boldsymbol u_{k}\\right) \\text {, }$\n",
    "\n",
    "**Our current grid world case is discrete** and system looks like $\\boldsymbol x_{k+1} = \\text{predictor}(\\boldsymbol x_{k}, \\boldsymbol u)=\\boldsymbol f(\\boldsymbol x_{k}, \\boldsymbol u)$. So, **predictor** is trivial here and just invokes a state transition rule, which is already implemented in the system.\n",
    "\n",
    "***\n",
    "\n",
    "### About state transition rules\n",
    "So, now we understand, that predictor is something that predicts the resulting state after our action is applied. But what are the rules? Here are:\n",
    "* If the agent in the terminal state, it goes nowhere and doesn't receive reward from any action\n",
    "* If the agent goes out of bounds -> it goes nowhere and just stay on its place and receives reward as usual\n",
    "* All other cases are intuitive:\n",
    "    * `predictor.predict((x, y), 0)` -> `(x + 1, y)`\n",
    "    * `predictor.predict((x, y), 1)` -> `(x - 1, y)`\n",
    "    * `predictor.predict((x, y), 2)` -> `(x, y + 1)`\n",
    "    * `predictor.predict((x, y), 3)` -> `(x, y - 1)`\n",
    "    * `predictor.predict((x, y), 4)` -> `(x, y)`\n",
    "\n",
    "Now, let's move on the \"how-to\" part.\n",
    "\n",
    "***\n",
    "\n",
    "### \"How-to\":\n",
    "* *How to get what i need? Where all classes and variables are stored?* -\n",
    "Since Rconita is created in a modular style, you almost always can see what's going on in a specific module, going hierarchically from the pipeline to right wherever you need. For example, to get the weights of critic model, here you go:\n",
    "```python\n",
    "pipeline.critic.model.weights\n",
    "```\n",
    "Or maybe just `critic.model.weights` if `critic` is available as class instance at your scope.\n",
    "\n",
    "* *How to obtain a prediction correctly?* - \n",
    "prediction of the next state can be obtained the following way:\n",
    "```python\n",
    "prediction = self.predictor.predict(observation, action)\n",
    "```\n",
    "In that case predictor will return a tuple with coordinates of the next state.\n",
    "\n",
    "* `self.discount_factor` - that's how you can obtain $\\gamma$ inside critic and actor\n",
    "* `self.running_objective(observation, action)` - that's how you can obtain a value $r(\\boldsymbol x, \\boldsymbol u)$\n",
    "* *What if i want to use critic inside of actor and vice versa?* - ACTOR IS AWARED OF CRITIC. As critic is awared of actor's model. When you work with actor, you can access **critic's weights** (and vice versa) and objective, don't dublicate your code\n",
    "* A **full list of parameters** explicitly passed into actor and critic can be found in the testing cell. There is an `initialize_actor_critic()` method, where those classes are instantiated.\n",
    "\n",
    "There are 3 problems in this assignment in total and many things related to implementation are being behind the scene. Your main objective is to implement 3 algorithms of tables (weights) updating procedure correctly. In case of successful implementation, you will obtain something like that:\n",
    "<img style=\"center\" src=\"grid.png\" width=58% height=58%>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f10ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here you go, try it\n",
    "\"\"\"\n",
    "action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b3d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\"\"\"\n",
    "Just importing all the necessary stuff here.\n",
    "DO NOT CHANGE\n",
    "\"\"\"\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from rcognita_framework.pipelines.pipeline_grid_world import PipelineTabular, action_space\n",
    "from rcognita_framework.rcognita.actors import ActorTabular\n",
    "from rcognita_framework.rcognita.critics import CriticTabularVI\n",
    "from rcognita_framework.rcognita.utilities import rc\n",
    "from copy import deepcopy #wow, this is a helpful easter egg\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a137d04",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\"> Section 1: Value iteration (VI) algorithm </h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd12ef3a",
   "metadata": {},
   "source": [
    "###  <font color=\"blue\"> 1.1 Value iteration (VI) algorithm mathematical description </font>\n",
    "\n",
    "The VI algorithm casts into following steps:\n",
    "\n",
    "I. **Initialization**:\n",
    "- set iterations number **N_iterations**\n",
    "- set **discount factor** $\\gamma$\n",
    "- initialize some **value** table $V_0$\n",
    "- initialize some **policy** table $\\rho_0$\n",
    "\n",
    "II. **Main loop**:<br/>\n",
    ">**for** i in range(**N_iterations**):\n",
    ">1. **policy update**:\n",
    "    >>$\\forall \\boldsymbol x$ do:\n",
    "    >>> $\\rho_i(x):=\\arg \\max _{u \\in \\mathbb{J}}\\left\\{r(x, u)+\\gamma V_i(f(x, u))\\right\\}$\n",
    ">    \n",
    ">2. **value update**:\n",
    "    >>$\\forall \\boldsymbol x$ do:\n",
    "    >>> $V_{i+1}(x):=r\\left(x, \\rho_i(x)\\right)+\\gamma V_i\\left(f\\left(x, \\rho_i(x)\\right)\\right)$\n",
    "\n",
    "***\n",
    "\n",
    "#### Your task is to implement main parts of this algorithm - update phases\n",
    "\n",
    "The following structure is ony a proposition. You can make any implementation you want, even with multiprocessing of whatever, go ahead. The main objective here is to implement correctly the `update` method, because it's being used in the update procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df25ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticTabularVIStudent(CriticTabularVI):\n",
    "    \"\"\"\n",
    "    This class is assumed to contain a table which forms the value function.\n",
    "    Therefore it contains methods for objective calculation and value function updating.\n",
    "    \"\"\"\n",
    "    def update_single_cell(self, observation):\n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "        \n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################\n",
    "\n",
    "    def update(self):\n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "        \n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################\n",
    "\n",
    "    def objective(self, action, observation):\n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "        \n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorTabularStudent(ActorTabular):\n",
    "    def update(self):\n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "        \n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2d5b37",
   "metadata": {},
   "source": [
    "###  <font color=\"blue\"> 1.2 Value iteration (VI) testing </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83518f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Launch this to try your algorithm in action\n",
    "\"\"\"\n",
    "class PipelineRLStudent(PipelineTabular):\n",
    "    def initialize_actor_critic(self):\n",
    "        self.critic = CriticTabularVIStudent(\n",
    "            dim_state_space=self.grid_size,\n",
    "            running_objective=self.running_objective,\n",
    "            predictor=self.predictor,\n",
    "            model=self.critic_model,\n",
    "            actor_model=self.actor_model,\n",
    "            discount_factor=self.discount_factor,\n",
    "            terminal_state=self.reward_cell,\n",
    "        )\n",
    "        self.actor = ActorTabularStudent(\n",
    "            dim_world=self.grid_size,\n",
    "            predictor=self.predictor,\n",
    "            optimizer=self.actor_optimizer,\n",
    "            running_objective=self.running_objective,\n",
    "            model=self.actor_model,\n",
    "            action_space=action_space,\n",
    "            critic=self.critic,\n",
    "            discount_factor=self.discount_factor,\n",
    "            terminal_state=self.reward_cell,\n",
    "        )\n",
    "\n",
    "pipeline_VI = PipelineRLStudent()\n",
    "pipeline_VI.execute_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd51a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LAUNCH THIS ONLY WHEN THE PREVIOUS ANIMATION IS DONE\n",
    "\"\"\"\n",
    "grade_VI = (abs((pipeline_VI.critic.model.weights[1,1] - 76.) < 1e-2) and \n",
    "            (abs(pipeline_VI.critic.model.weights[3,3] + 5.)<1e-2)) * 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6527d4",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\"> Section 2: Policy iteration </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da799767",
   "metadata": {},
   "source": [
    "###  <font color=\"blue\"> Value iteration (VI) algorithm mathematical description </font>\n",
    "\n",
    "The VI algorithm casts into following steps:\n",
    "\n",
    "I. **Initialization**:\n",
    "- set iterations number **N_iterations**\n",
    "- set **discount factor** $\\gamma$\n",
    "- set **tollerance** for value updating procedure\n",
    "- set **N_update_iters_max** for value updating procedure to prevent an infiniteness of the loop execution\n",
    "- initialize some **value** table $V_0$\n",
    "- initialize some **policy** table $\\rho_0$\n",
    "\n",
    "II. **Main loop**:<br/>\n",
    "> **for** i in **N_iterations**:\n",
    ">\n",
    "> 1. **value update**:\n",
    "    >>$\\forall \\boldsymbol x$ do:\n",
    "    >>> **for** j in range(1, **N_update_iters_max**):\n",
    "    >>>>\n",
    "    >>>> $V^{j}_{i}(x) - V^{j-1}_{i}(x) = \\text{difference}_j$\n",
    "    >>>>\n",
    "    >>>> **if** $\\text{difference}_j$ > **tolerance**:\n",
    "    >>>>> $V^{j}_{i}(x):=r\\left(x, \\rho_i(x)\\right)+\\gamma V_i\\left(f\\left(x, \\rho_i(x)\\right)\\right)$\n",
    "    >>>>\n",
    "    >>>> **else**:\n",
    "    >>>>> **break**\n",
    ">        \n",
    ">\n",
    "> 2. **policy update**:\n",
    "    >>$\\forall \\boldsymbol x$ do:\n",
    "    >>> $\\rho_{i+1}(x):=\\arg \\max _{u \\in \\mathbb{J}}\\left\\{r(x, u)+\\gamma V_i(f(x, u))\\right\\}$\n",
    "\n",
    "***\n",
    "\n",
    "#### Your task is to implement main parts of this algorithm - update phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticTabularPIStudent(CriticTabularVIStudent):\n",
    "    def __init__(self, *args, tolerance=1e-3, N_update_iters_max=50, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tolerance = tolerance\n",
    "        self.N_update_iters_max = N_update_iters_max\n",
    "\n",
    "    def update(self):\n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "        \n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ab218",
   "metadata": {},
   "source": [
    "###  <font color=\"blue\"> 2.3 Policy iteration (PI) testing </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a401bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineRLStudent(PipelineTabular):\n",
    "    def initialize_actor_critic(self):\n",
    "        \"\"\"\n",
    "        Uncomment this to go into debug mode. \n",
    "        You will be able to see a full traceback under the static picture of grid.\n",
    "        \"\"\"\n",
    "        #self.no_visual = True\n",
    "        self.critic = CriticTabularPIStudent(\n",
    "            dim_state_space=self.grid_size,\n",
    "            running_objective=self.running_objective,\n",
    "            predictor=self.predictor,\n",
    "            model=self.critic_model,\n",
    "            actor_model=self.actor_model,\n",
    "            discount_factor=self.discount_factor,\n",
    "            terminal_state=self.reward_cell,\n",
    "        )\n",
    "        self.actor = ActorTabularStudent(\n",
    "            dim_world=self.grid_size,\n",
    "            predictor=self.predictor,\n",
    "            optimizer=self.actor_optimizer,\n",
    "            running_objective=self.running_objective,\n",
    "            model=self.actor_model,\n",
    "            action_space=action_space,\n",
    "            critic=self.critic,\n",
    "            discount_factor=self.discount_factor,\n",
    "            terminal_state=self.reward_cell,\n",
    "        )\n",
    "\n",
    "pipeline_PI = PipelineRLStudent()\n",
    "pipeline_PI.execute_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35735ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LAUNCH THIS ONLY WHEN THE PREVIOUS ANIMATION IS DONE\n",
    "\"\"\"\n",
    "grade_PI = (abs((pipeline_PI.critic.model.weights[1,1] - 76.) < 1e-2) and \n",
    "            (abs(pipeline_PI.critic.model.weights[3,3] + 5.)<1e-2)) * 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84536385",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\"> Section 3: Q-learning </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2260d775",
   "metadata": {},
   "source": [
    "###  <font color=\"blue\"> 3.1 Q-learning (QL) algorithm mathematical description </font>\n",
    "\n",
    "The QL algorithm casts into following steps:\n",
    "\n",
    "I. **Initialization**:\n",
    "- set iterations number **N_iterations**\n",
    "- set **discount factor** $\\gamma$\n",
    "- initialize some **Q** table $Q_0$\n",
    "- initialize some **policy** table $\\rho_0$\n",
    "\n",
    "II. **Main loop**:<br/>\n",
    ">**for** i in range(**N_iterations**):\n",
    ">\n",
    "> 1. **policy update**:\n",
    "    >>$\\forall \\boldsymbol x$ do:\n",
    "    >>> $\\rho_i(x): =\\arg \\max _{u \\in J} Q_i(x, u)$\n",
    ">    \n",
    "> 2. **Q-function update**:\n",
    "    >>$\\forall \\boldsymbol x$ do:\n",
    "    >>> $Q_{i+1}(x, u): =r(x, u)+\\gamma Q_i\\left(f(x, u), \\rho_i(f(x, u))\\right)$\n",
    "\n",
    "#### Your task is to implement main parts of this algorithm - update phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2633bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticTabularQLStudent(CriticTabularVI):\n",
    "    \"\"\"\n",
    "    This class is assumed to contain a table which forms the Q-function.\n",
    "    Therefore it contains methods for objective calculation and Q-function updating.\n",
    "    \"\"\"\n",
    "    def update_single_cell(self, observation):\n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "        \n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################\n",
    "\n",
    "    def update(self):\n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "        \n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################\n",
    "\n",
    "    def objective(self, observation, action):\n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "        \n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d073dbc7",
   "metadata": {},
   "source": [
    "###  <font color=\"blue\"> 3.2 Q-learning (QL) testing </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b280176",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineRLStudent(PipelineTabular):\n",
    "    def initialize_actor_critic(self):\n",
    "        \"\"\"\n",
    "        Uncomment this to go into debug mode. \n",
    "        You will be able to see a full traceback under the static picture of grid.\n",
    "        \"\"\"\n",
    "        #self.no_visual = True\n",
    "        self.critic = CriticTabularQLStudent(\n",
    "            dim_state_space=self.grid_size,\n",
    "            running_objective=self.running_objective,\n",
    "            predictor=self.predictor,\n",
    "            model=self.critic_model,\n",
    "            actor_model=self.actor_model,\n",
    "            discount_factor=self.discount_factor,\n",
    "            terminal_state=self.reward_cell,\n",
    "        )\n",
    "        self.actor = ActorTabularStudent(\n",
    "            dim_world=self.grid_size,\n",
    "            predictor=self.predictor,\n",
    "            optimizer=self.actor_optimizer,\n",
    "            running_objective=self.running_objective,\n",
    "            model=self.actor_model,\n",
    "            action_space=action_space,\n",
    "            critic=self.critic,\n",
    "            discount_factor=self.discount_factor,\n",
    "            terminal_state=self.reward_cell,\n",
    "        )\n",
    "\n",
    "pipeline_QL = PipelineRLStudent()\n",
    "pipeline_QL.execute_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac436493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LAUNCH THIS ONLY WHEN THE PREVIOUS ANIMATION IS DONE\n",
    "\"\"\"\n",
    "grade_QL = (abs((pipeline_QL.critic.model.weights[1,1] - 76.) < 1e-2) and \n",
    "            (abs(pipeline_QL.critic.model.weights[3,3] + 5.)<1e-2)) * 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65e6d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here are your expected total points\n",
    "\"\"\"\n",
    "print(f\"Your total grade: {grade_VI+grade_PI+grade_QL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1bb577",
   "metadata": {},
   "source": [
    "In case if you sruggle with your assignment, just PM him on telegram ðŸ˜Š -> @odinmaniac"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
