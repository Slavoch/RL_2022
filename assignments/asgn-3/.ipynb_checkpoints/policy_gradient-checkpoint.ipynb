{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c715e6",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#333333; text-align:center; line-height: 0;\"> <img style=\"right;\" src=\"logo.png\" width=18% height=18%> Reinforcement Learning | Assignment 3 \n",
    "</h1>\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "The goal of this assignment is to implement:\n",
    "- Critic\n",
    "- PyTorch optimizer\n",
    "- Actor-Critic algorithm\n",
    "\n",
    "___Total points:___ 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96415eab",
   "metadata": {},
   "source": [
    "###  <font color=\"blue\"> A brief introduction </font>\n",
    "Examine it carefully, it covers most of your possible needs to make an assignment.\n",
    "\n",
    "***\n",
    "\n",
    "### About Rcognita\n",
    "The platform for this (and all subsequent work) is [Rcognita](https://gitflic.ru/project/aidynamicaction/rcognita), a framework for applying control theory and machine learning algorithms to control problems, an integral part of which is the closed-loop interaction between the agent under control and the environment evolving over time. In the Rcognita paradigm, the main bearer of all the classes and variables needed to run the simulation is the `pipeline`. \n",
    "\n",
    "The main parts of `pipeline` are: \n",
    "* `simulator`, which is defined at module `simulators.py` and responsible for simulation of evolution of the environment\n",
    "* `actor`, defined at module `actors.py`, which is responsible for obtaining of action\n",
    "* `critic`, defined at module `critics.py`, which is reponsible for learning of reward function and obtaining its value \n",
    "* `controller`, which is defined at module `controllers.py` and it's needed to put it all together into an RL (or other) controller\n",
    "* `system`, which is defined at module `systems.py`.\n",
    "\n",
    "Other minor things are also declarated in the pipeline and assembled module by module up to the execution of the pipeline itself. \n",
    "Just to be on the same page, we provide some notation to prevent further confusions.\n",
    "* `weights` is the general name and for weights of neural network and for values in tables of value function and policy as well. This agreement comes from the motivation for being consistent with classical RL where critic and actor are being implemented as some neural networks with some **weights**. So, here comes the second term\n",
    "* `model`. It's obvious that parameters give specificity to something. But the general form itself is being called `model`. There are plenty of models of different types and forms (such as NN). Model is what critic and actor and even running cost always have, no matter what.\n",
    "* `predictor` - Inspite of it's cryptic name, this object performs an important function, namely, it carries the law by which the dynamics of our system is being predicted in future. For example, if we have some differential equation\n",
    "$\n",
    "\\begin{cases}\n",
    "\\dot{\\boldsymbol x} = \\boldsymbol f(\\boldsymbol x, \\boldsymbol u)\\\\\n",
    "\\boldsymbol y = h(\\boldsymbol x) \\\\\n",
    "\\boldsymbol x(0)=\\boldsymbol x_{0}\\\\\n",
    "\\end{cases}\n",
    "$\n",
    "where $x_{0}$ is the **initial state**.\n",
    "in general, there are several ways of prediction: \n",
    "> - **Analytical**, when we have a precise formula of analytical solution $\\boldsymbol x(t)$ to the ODE and have no problems to compute it at any given time. This is great but not that possible in real life. Nevertheless, our predictor could be expressed like:  $\\text{predictor}(\\boldsymbol x(\\tau),dt) = \\boldsymbol x(\\tau + dt)$\n",
    "> - **Numerical** way is mostly a case. The simplest way of prediction then is an Euler method:\n",
    "$\\boldsymbol x_{k+1}= \\text{predictor}(\\boldsymbol x_k, \\delta)=\\boldsymbol x_{k}+\\delta \\boldsymbol f\\left(\\boldsymbol x_{k}, \\boldsymbol u_{k}\\right) \\text {, }$\n",
    "\n",
    "In this assignment we meet a new object - **scenario**. Scenario is a module that forms and executes the main loops for different scenarios, like online or episodical scenario. In this assignment we will use an episodical scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421bd659",
   "metadata": {},
   "source": [
    "<a id='Notation'></a>\n",
    "### Notation summary\n",
    "From now and on we will use the following notation:\n",
    "\n",
    "| Notation &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;&nbsp;Description |\n",
    "|:-----------------------:|-------------|\n",
    "| $\\boldsymbol f(\\cdot, \\cdot, \\cdot) : \\mathbb{R}^{n+1}\\times \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{n}$ |A **state dynamic function** or, more informally, **righ-hand-side** of a system <br /> of ordinary differential equations $\\dot{\\boldsymbol x} = \\boldsymbol f(t, \\boldsymbol x, \\boldsymbol u)$|\n",
    "| $\\boldsymbol x \\in \\mathbb{R}^{n} $ | An element of the **state space** of a controlled system of dimensionality $n$ |\n",
    "| $\\boldsymbol u \\in \\mathbb{R}^{m}$ | An element of the **action space** of a controlled system of dimensionality $m$ |\n",
    "| $\\boldsymbol y \\in \\mathbb{R}^{k}$ | An **observartion**|\n",
    "| $\\mathbb{X}\\subset \\mathbb{R}^{n} $| **State constraint set**|\n",
    "| $\\mathbb{U}\\subset \\mathbb{R}^{m} $| **Action constraint set**|\n",
    "| $\\boldsymbol h(\\cdot): \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{k}$ | **Observation function**  |\n",
    "| $\\boldsymbol\\rho(\\cdot) : \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ | **Policy** function |\n",
    "| $r(\\cdot) : \\mathbb{R}^n \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}$ | **Running cost** function  |\n",
    "\n",
    "\n",
    "### Goal\n",
    "Our main goal here is to implement Actor-Critic algorithm to [PID-regulator](https://en.wikipedia.org/wiki/PID_controller) coefficients tuning\n",
    "\n",
    "###  <font color=\"blue\"> Algorithm description </font>\n",
    "\n",
    "In this setup we will interchange a trivial critic with the \"action-value\" one. The purpose of this critic is to learn the $Q$-function, so the algorithm will be constructed as follows:\n",
    "\n",
    "I. **Initialization**:\n",
    "- set iterations number **N_iterations**\n",
    "- set episodes number **N_episodes**\n",
    "- set **discount factor** $\\gamma$\n",
    "- initialize some **policy** parameters $\\boldsymbol w_0$, learning rate $\\eta$\n",
    "- initialize some **critic** parameters $\\boldsymbol \\vartheta_0$, learning rate $\\hat{\\eta}$\n",
    "\n",
    "II. **Main loop**:<br/>\n",
    "(Run episodical scenario)\n",
    ">**for** i in range(**N_iterations**):\n",
    ">>**for** j in range(**N_episodes**):\n",
    ">>> **while** **time** < **t1**:\n",
    "(corresponding utilized parts of Rcognita are provided in bold inside parentheses)\n",
    ">>>> - simulate environment evolution (**simulator**, **system**)\n",
    ">>>> - obtain observation (**system**) $\\boldsymbol y_i = \\boldsymbol h(\\boldsymbol x_i)$\n",
    ">>>> - obtain action (**actor**) $u_i \\sim  \\mathcal{N}(\\mu,\\,\\sigma^{2})$\n",
    ">>>> - compute and store new gradient (**actor.model**)\n",
    ">>>  - **reset episode:**\n",
    ">>>> - compute and store REINFORCE objective gradient (**scenario**): $\\sum_{k=0}^N \\nabla_w \\ln \\rho^w(\\boldsymbol u_k \\vert \\boldsymbol y_k) \\cdot Q\\left(\\boldsymbol y_k, \\boldsymbol u_k\\right)$\n",
    ">>>> - compute and store sum of squared Temporal Difference terms (scenario): $\\sum_{k=0}^N \\text{TD}_k^2$\n",
    ">> - **iteration update:**\n",
    ">>> - **critic update**\n",
    ">>>> - compute **mean**(`squared_TD_sums_of_episodes`) = $\\mathbb{E}[\\sum_{k=0}^N \\text{TD}_k^2]$ - mean of saquared TD by episodes\n",
    ">>>> - $\\boldsymbol \\vartheta_{i+1} = \\boldsymbol \\vartheta_{i} - \\hat{\\eta}\\nabla_{\\boldsymbol \\vartheta} \\mathbb{E}[\\sum_{k=0}^N \\text{TD}_k^2]$ (1 step)\n",
    ">>> - **actor update**\n",
    ">>>> - compute **mean** overall stored REINFORCE objective gradients (**scenario**): $\\mathbb{E}\\left[\\sum_{k=0}^N \\nabla_{\\boldsymbol w} \\ln \\rho^{\\boldsymbol w}(\\boldsymbol u_k \\vert \\boldsymbol y_k) \\cdot Q\\left(\\boldsymbol y_k, \\boldsymbol u_k\\right)\\right]$\n",
    ">>>> - perform a gradient step (**scenario**): $\\boldsymbol w_{i+1}=\\boldsymbol w_i+\\eta \\mathbb{E}\\left[\\sum_{k=0}^N \\nabla_{\\boldsymbol w} \\ln \\rho^{\\boldsymbol w}(\\boldsymbol u_k \\vert \\boldsymbol y_k) \\cdot Q\\left(\\boldsymbol y_i, \\boldsymbol u_i\\right)\\right]$\n",
    ">> - **reset iteration** ...\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9870459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\"\"\"\n",
    "Just importing all the necessary stuff here.\n",
    "DO NOT CHANGE\n",
    "\"\"\"\n",
    "%matplotlib qt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from rcognita_framework.pipelines.pipeline_inverted_pendulum import PipelineInvertedPendulum\n",
    "from rcognita_framework.rcognita.actors import ActorProbabilisticEpisodic\n",
    "from rcognita_framework.rcognita.critics import CriticActionValue\n",
    "from rcognita_framework.rcognita.systems import SysInvertedPendulum\n",
    "from rcognita_framework.rcognita.models import ModelGaussianConditional, ModelNN\n",
    "from rcognita_framework.rcognita.scenarios import EpisodicScenario\n",
    "from rcognita_framework.rcognita.utilities import rc\n",
    "from rcognita_framework.rcognita.optimizers import BaseOptimizer\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039587ee",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\"> Section 1: Critic implementation </h2>\n",
    "\n",
    "***\n",
    "Contents:\n",
    "* Model implementation\n",
    "* Critic implementation\n",
    "\n",
    "***\n",
    "\n",
    "Implement your topology here. You can try out pass `(input tensor ** 2)` into linear layer. Why is that? Because one may observe that critic should be semi-negative-definite in our case. In other words, `critic([0,0,0],[0])` should be zero and `critic(y, u)` < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49c9c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ModelNNStudent(ModelNN):\n",
    "\n",
    "    model_name = \"NN\"\n",
    "\n",
    "    def __init__(self, dim_observation, dim_action, *args, weights = None, **kwargs):\n",
    "        super().__init__(dim_observation, dim_action, *args, weights=weights, **kwargs)\n",
    "        \n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "\n",
    "        self.fc1 = nn.Linear(\n",
    "            dim_observation + dim_action, dim_observation + dim_action, bias=False\n",
    "        )\n",
    "\n",
    "        if weights is not None:\n",
    "            self.load_state_dict(weights)\n",
    "            \n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################\n",
    "\n",
    "        self.double()\n",
    "        self.cache_weights()\n",
    "\n",
    "    def forward(self, input_tensor, weights=None):\n",
    "        if weights is not None:\n",
    "            self.update(weights)\n",
    "        \n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "\n",
    "        x = input_tensor\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x = -(x ** 2)\n",
    "        x = torch.sum(x)\n",
    "        \n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbabcd39",
   "metadata": {},
   "source": [
    "### Q-critic (Action-Value-critic) implementation\n",
    "\n",
    "Here we will implement a temporal difference.\n",
    "FYI:\n",
    "* Vectores in the data buffer are stored in the order from the latest (top, vectors indiced as `[0,:]`) to the  recent one (bottom, `[-1, :]`)\n",
    "* Data buffer length corresponds to `self.data_buffer_size`\n",
    "* Important reminder: all models have an ability to cache themselves. Here you can use this to evaluate a TD correctly. Just set `use_stored_weights=True` when you call the critic's model. It will invoke cached weights that also **were detached automatically**. So, you can construct a temporal difference without using Torch functionality  directly. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "841be197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.6767, dtype=torch.float64, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelNNStudent(3, 1)\n",
    "model([1.,2.,3.], [1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0de7ada5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.6767, dtype=torch.float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model([1.,2.,3.], [1.], use_stored_weights=True) #### gradient won't flow through this tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c36dec",
   "metadata": {},
   "source": [
    "Note, how the first input is different from the second one. Okay, let's move on!\n",
    "* Last, but not least! $\\text{TD}(\\boldsymbol y_{\\text{old}},y_{\\text{next}},a_{\\text{old}},a_{\\text{next}})= \\text{critic}(y_{\\text{old}}, a_{\\text{old}}) - \\text{critic}^*(y_{\\text{next}}, a_{\\text{next}})$, where $\\text{critic}^*$ is a critic with fixed weights!.\n",
    "\n",
    "* Function `objective` should return sum of all possible squared TDs given the current data buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2f08857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticActionValue(CriticActionValue):\n",
    "    def objective(self, data_buffer=None, weights=None):\n",
    "        \"\"\"\n",
    "        Objective of the critic, say, a squared temporal difference.\n",
    "\n",
    "        \"\"\"\n",
    "        if data_buffer is None:\n",
    "            observation_buffer = self.observation_buffer\n",
    "            action_buffer = self.action_buffer\n",
    "        else:\n",
    "            observation_buffer = data_buffer[\"observation_buffer\"]\n",
    "            action_buffer = data_buffer[\"action_buffer\"]\n",
    "\n",
    "        critic_objective = 0\n",
    "        \n",
    "        ####### At this point the data buffer is available, just use it #######\n",
    "        \n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "\n",
    "        for k in range(self.data_buffer_size - 1, 0, -1):\n",
    "            observation_old = observation_buffer[k - 1, :]\n",
    "            observation_next = observation_buffer[k, :]\n",
    "            action_old = action_buffer[k - 1, :]\n",
    "            action_next = action_buffer[k, :]\n",
    "\n",
    "            # Temporal difference\n",
    "\n",
    "            critic_old = self.model(observation_old, action_old, weights=weights)\n",
    "            critic_next = self.model(\n",
    "                observation_next, action_next, use_stored_weights=True\n",
    "            )\n",
    "\n",
    "            temporal_difference = (\n",
    "                critic_old\n",
    "                - self.discount_factor * critic_next\n",
    "                - self.running_objective(observation_old, action_old)\n",
    "            )\n",
    "\n",
    "            critic_objective += 1 / 2 * temporal_difference ** 2\n",
    "            \n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################\n",
    "\n",
    "        return critic_objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c471d72",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\"> Section 2: Actor modification </h2>\n",
    "\n",
    "As you might remember from the last assignment, you've implemented the evaluation of the policy distribution gradient in the `update` Actor's method. Now the only thing we should do is change it a bit. \n",
    "\n",
    "Take the `update` function from your last solution, insert it here and multiply your gradient by Q-function using your critic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb8b6531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorProbabilisticEpisodicAC(ActorProbabilisticEpisodic):\n",
    "    def update(self, observation):\n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "        action_sample = self.model.sample_from_distribution(observation)\n",
    "        self.action = np.array(\n",
    "            np.clip(action_sample, self.action_bounds[0], self.action_bounds[1])\n",
    "        )\n",
    "        self.action_old = self.action\n",
    "\n",
    "        Q_value = self.critic(observation, action_sample).detach().numpy()\n",
    "        current_gradient = self.model.compute_gradient(action_sample) * Q_value\n",
    "\n",
    "        self.store_gradient(current_gradient)\n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9857ca",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\"> Section 3: Optimizer implementation </h2>\n",
    "\n",
    "Torch optimization loop should be implemented here.\n",
    "1. Zero all gradients\n",
    "2. Compute loss with objective function you've passed\n",
    "3. Invoke gradients evaluation with .backward()\n",
    "4. perform optimization step with .step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3a463ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchOptimizer(BaseOptimizer):\n",
    "    engine = \"Torch\"\n",
    "\n",
    "    def __init__(\n",
    "        self, opt_options, iterations=1, opt_method=torch.optim.Adam, verbose=False\n",
    "    ):\n",
    "        self.opt_method = opt_method\n",
    "        self.opt_options = opt_options\n",
    "        self.iterations = iterations\n",
    "        self.verbose = verbose\n",
    "        self.loss_history = []\n",
    "\n",
    "    def optimize(self, objective, model, objective_input):\n",
    "        optimizer = self.opt_method(\n",
    "            model.parameters(), **self.opt_options, weight_decay=0\n",
    "        )\n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "        for _ in range(self.iterations):\n",
    "            optimizer.zero_grad()\n",
    "            loss = objective(objective_input)\n",
    "            loss_before = loss.detach().numpy()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c68ec2",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\"> Section 4: Scenario implementation</h2>\n",
    "\n",
    "***\n",
    "\n",
    "In this task you will just modify the scenario you've touched in the previous assignment. This is an elementary task that is needed just to make you think about what is happening at the end of episode and iteration. \n",
    "* At the end of the episode you should append a critic objective value to `squared_TD_sums_of_episodes` list\n",
    "* in the iteration update phase you launch optimizer\n",
    "* when you reset the iteration, you just empty `squared_TD_sums_of_episodes`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e1d21d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicScenarioAsyncAC(EpisodicScenario):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.critic_optimizer = TorchOptimizer({\"lr\": 0.01})\n",
    "        self.squared_TD_sums_of_episodes = []\n",
    "        self.square_TD_means = []\n",
    "\n",
    "    def reset_episode(self):\n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "        self.squared_TD_sums_of_episodes.append(self.critic.objective())\n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################\n",
    "        super().reset_episode()\n",
    "\n",
    "    def iteration_update(self):\n",
    "        mean_sum_of_squared_TD = self.get_mean(self.squared_TD_sums_of_episodes) #just for visualization purposes\n",
    "        self.square_TD_means.append(mean_sum_of_squared_TD.detach().numpy()) #just for visualization purposes\n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "        self.critic_optimizer.optimize(\n",
    "            objective=self.get_mean,\n",
    "            model=self.critic.model,\n",
    "            objective_input=self.squared_TD_sums_of_episodes,\n",
    "        )\n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################\n",
    "        super().iteration_update()\n",
    "\n",
    "    def reset_iteration(self):\n",
    "        #############################################\n",
    "        # YOUR CODE BELOW\n",
    "        #############################################\n",
    "        self.squared_TD_sums_of_episodes = []\n",
    "        #############################################\n",
    "        # YOUR CODE ABOVE\n",
    "        #############################################\n",
    "        super().reset_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e4bf4b",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\"> Section 5: Testing</h2>\n",
    "\n",
    "***\n",
    "\n",
    "Here you have a full freedom of choice: you can tune whatever you want, change whatever you want. Your goal here is to beat a baseline. If you get an outcome g.t. -350, you earn 100 points. If your result lower than -700, you earn nothing.\n",
    "Play with hyperparameters, choose number of episodes and number of iterations. You may also vary the length of one episode. There is plenty of work. Applying is not that straightforward, we made some necessary stuff but your main objective here is to apply your ML an mathematical intuition to obtain the best result you can. You will definitely have some questions. So do not hesitate to DM me on telegram ðŸ˜Š -> @odinmaniac\n",
    "\n",
    "Some addendums:\n",
    "* The problem is pretty stochastic, so sometimes you can occasionally obtain some good results. But the point here is to achieve a convergence! So, if you didn't obtain a stabilization of parameters and loss, it doesn't count (you will be able to see it on 3-rd and 4-th subplot). So, if you think that you obtained some solid results, make plots please, or ask me to launch your notebook if you struggle with hardware or software issues\n",
    "* If you're desperate, results are bad and you don't know what to do, you could try the following heuristics:\n",
    "    * disable the optimization of parameter I (the second one - $\\theta_2$)\n",
    "    * Change learning rate (sometimes you need to change is really drammatically, so it's okay)\n",
    "    * Bound your weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1f93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineInvertedPendulumStudent(PipelineInvertedPendulum):\n",
    "\n",
    "    def initialize_system(self):\n",
    "        self.system = SysInvertedPendulumStudent(\n",
    "            sys_type=\"diff_eqn\",\n",
    "            dim_state=self.dim_state,\n",
    "            dim_input=self.dim_input,\n",
    "            dim_output=self.dim_output,\n",
    "            dim_disturb=self.dim_disturb,\n",
    "            pars=[self.m, self.g, self.l],\n",
    "            is_dynamic_controller=self.is_dynamic_controller,\n",
    "            is_disturb=self.is_disturb,\n",
    "            pars_disturb=[],\n",
    "        )\n",
    "        self.observation_init = self.system.out(self.state_init, time=0)\n",
    "\n",
    "    def initialize_models(self):\n",
    "        super().initialize_models()\n",
    "        self.actor_model = ModelGaussianConditionalStudent(\n",
    "            expectation_function=self.safe_controller,\n",
    "            arg_condition=self.observation_init,\n",
    "            weights=self.initial_weights,\n",
    "        )\n",
    "\n",
    "    def initialize_actor_critic(self):\n",
    "        self.critic = CriticTrivialStudent(\n",
    "            running_objective=self.running_objective, sampling_time=self.sampling_time\n",
    "        )\n",
    "        self.actor = ActorProbabilisticEpisodicStudent(\n",
    "            self.prediction_horizon,\n",
    "            self.dim_input,\n",
    "            self.dim_output,\n",
    "            self.control_mode,\n",
    "            self.action_bounds,\n",
    "            action_init=self.action_init,\n",
    "            predictor=self.predictor,\n",
    "            optimizer=self.actor_optimizer,\n",
    "            critic=self.critic,\n",
    "            running_objective=self.running_objective,\n",
    "            model=self.actor_model,\n",
    "        )\n",
    "\n",
    "    def initialize_scenario(self):\n",
    "        self.scenario = EpisodicScenarioStudent(\n",
    "            system=self.system,\n",
    "            simulator=self.simulator,\n",
    "            controller=self.controller,\n",
    "            actor=self.actor,\n",
    "            critic=self.critic,\n",
    "            logger=self.logger,\n",
    "            datafiles=self.datafiles,\n",
    "            time_final=self.time_final,\n",
    "            running_objective=self.running_objective,\n",
    "            no_print=self.no_print,\n",
    "            is_log=self.is_log,\n",
    "            is_playback=self.is_playback,\n",
    "            N_episodes=self.N_episodes,\n",
    "            N_iterations=self.N_iterations,\n",
    "            state_init=self.state_init,\n",
    "            action_init=self.action_init,\n",
    "        )\n",
    "\n",
    "    def execute_pipeline(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Full execution routine\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        self.load_config()\n",
    "        self.setup_env()\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.initialize_system()\n",
    "        self.initialize_predictor()\n",
    "        self.initialize_safe_controller()\n",
    "        self.initialize_models()\n",
    "        self.initialize_objectives()\n",
    "        self.initialize_optimizers()\n",
    "        self.initialize_actor_critic()\n",
    "        self.initialize_controller()\n",
    "        self.initialize_simulator()\n",
    "        self.initialize_logger()\n",
    "        self.initialize_scenario()\n",
    "        if not self.no_visual and not self.save_trajectory:\n",
    "            self.initialize_visualizer()\n",
    "            self.main_loop_visual()\n",
    "        else:\n",
    "            self.scenario.run()\n",
    "            if self.is_playback:\n",
    "                self.playback()\n",
    "                \n",
    "    #def playback(self):\n",
    "    #    self.initialize_visualizer()\n",
    "    #    anm = animation.FuncAnimation(\n",
    "    #        self.animator.fig_sim,\n",
    "    #        self.animator.playback,\n",
    "    #        init_func=self.animator.init_anim,\n",
    "    #        blit=False,\n",
    "    #        interval=self.sampling_time / 1e6,\n",
    "    #        repeat=False,\n",
    "    #    )\n",
    "#\n",
    "    #    self.animator.get_anm(anm)\n",
    "    #    self.animator.speedup = self.speedup\n",
    "#\n",
    "    #    cId = self.animator.fig_sim.canvas.mpl_connect(\n",
    "    #        \"key_press_event\", lambda event: on_key_press(event, anm)\n",
    "    #    )\n",
    "#\n",
    "    #    anm.running = True\n",
    "#\n",
    "    #    self.animator.fig_sim.tight_layout()\n",
    "    #    plt.show()\n",
    "    \n",
    "##### Execution here!!! Full list of kwargs can be seen at \n",
    "##### rcognita_framework.pipelines.config_blueprints in the ConfigInvertedPendulum\n",
    "pipeline = PipelineInvertedPendulumStudent()\n",
    "pipeline.execute_pipeline(\n",
    "    no_visual=True, \n",
    "    time_final=10, \n",
    "    speedup=50,\n",
    "    is_playback=True, \n",
    "    N_episodes=3, \n",
    "    N_iterations=8, \n",
    "    learning_rate=0.01,\n",
    "    initial_weights=[1., 0., 1.],\n",
    "    sampling_time=0.1, # Do not change it!\n",
    "    no_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6317032",
   "metadata": {},
   "source": [
    "### Grading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f597b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = PipelineInvertedPendulumStudent()\n",
    "pipeline.execute_pipeline(\n",
    "    no_visual=True, \n",
    "    t1=10, \n",
    "    is_playback=False, \n",
    "    N_episodes= , ##### set your episodes number \n",
    "    N_iterations=1,##### set your iterations number \n",
    "    initial_weights=[1., 0., 1.],\n",
    "    no_print=True)\n",
    "\n",
    "mean_episodic = pipeline.scenario.outcome_episodic_means[0]\n",
    "grade = np.clip(0.28 * mean_episodic + 200, 0, 100)\n",
    "print(f\"Your grade: {grade}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
