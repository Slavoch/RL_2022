{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c715e6",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#333333; text-align:center; line-height: 0;\"> <img style=\"right;\" src=\"logo.png\" width=18% height=18%> Reinforcement Learning | Assignment 1 \n",
    "</h1>\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "The goal of this assignment is to implement:\n",
    "- system \n",
    "- conditional model \n",
    "- actor \n",
    "- critic\n",
    "- REINFORCE\n",
    "\n",
    "___Total points:___ 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96415eab",
   "metadata": {},
   "source": [
    "###  <font color=\"blue\"> A brief introduction </font>\n",
    "Examine it carefully, it covers most of your possible needs to make an assignment.\n",
    "\n",
    "***\n",
    "\n",
    "### About Rcognita\n",
    "The platform for this (and all subsequent work) is [Rcognita](https://gitflic.ru/project/aidynamicaction/rcognita), a framework for applying control theory and machine learning algorithms to control problems, an integral part of which is the closed-loop interaction between the agent under control and the environment evolving over time. In the Rcognita paradigm, the main bearer of all the classes and variables needed to run the simulation is the `pipeline`. \n",
    "\n",
    "The main parts of `pipeline` are: \n",
    "* `simulator`, which is defined at module `simulators.py` and responsible for simulation of evolution of the environment\n",
    "* `actor`, defined at module `actors.py`, which is responsible for obtaining of action\n",
    "* `critic`, defined at module `critics.py`, which is reponsible for learning of reward function and obtaining its value \n",
    "* `controller`, which is defined at module `controllers.py` and it's needed to put it all together into an RL (or other) controller\n",
    "* `system`, which is defined at module `systems.py`.\n",
    "\n",
    "Other minor things are also declarated in the pipeline and assembled module by module up to the execution of the pipeline itself. \n",
    "Just to be on the same page, we provide some notation to prevent further confusions.\n",
    "* `weights` is the general name and for weights of neural network and for values in tables of value function and policy as well. This agreement comes from the motivation for being consistent with classical RL where critic and actor are being implemented as some neural networks with some **weights**. So, here comes the second term\n",
    "* `model`. It's obvious that parameters give specificity to something. But the general form itself is being called `model`. There are plenty of models of different types and forms (such as NN). Model is what critic and actor and even running cost always have, no matter what.\n",
    "* `predictor` - Inspite of it's cryptic name, this object performs an important function, namely, it carries the law by which the dynamics of our system is being predicted in future. For example, if we have some differential equation\n",
    "$\n",
    "\\begin{cases}\n",
    "\\dot{\\boldsymbol x} = \\boldsymbol f(\\boldsymbol x, \\boldsymbol u)\\\\\n",
    "\\boldsymbol y = h(\\boldsymbol x) \\\\\n",
    "\\boldsymbol x(0)=\\boldsymbol x_{0}\\\\\n",
    "\\end{cases}\n",
    "$\n",
    "where $x_{0}$ is the **initial state**.\n",
    "in general, there are several ways of prediction: \n",
    "> - **Analytical**, when we have a precise formula of analytical solution $\\boldsymbol x(t)$ to the ODE and have no problems to compute it at any given time. This is great but not that possible in real life. Nevertheless, our predictor could be expressed like:  $\\text{predictor}(\\boldsymbol x(\\tau),dt) = \\boldsymbol x(\\tau + dt)$\n",
    "> - **Numerical** way is mostly a case. The simplest way of prediction then is an Euler method:\n",
    "$\\boldsymbol x_{k+1}= \\text{predictor}(\\boldsymbol x_k, \\delta)=\\boldsymbol x_{k}+\\delta \\boldsymbol f\\left(\\boldsymbol x_{k}, \\boldsymbol u_{k}\\right) \\text {, }$\n",
    "\n",
    "In this assignment we meet a new object - **scenario**. Scenario is a module that forms and executes the main loops for different scenarios, like online or episodical scenario. In this assignment we will use an episodical scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421bd659",
   "metadata": {},
   "source": [
    "<a id='Notation'></a>\n",
    "### Notation summary\n",
    "From now and on we will use the following notation:\n",
    "\n",
    "| Notation &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;&nbsp;Description |\n",
    "|:-----------------------:|-------------|\n",
    "| $\\boldsymbol f(\\cdot, \\cdot, \\cdot) : \\mathbb{R}^{n+1}\\times \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{n}$ |A **state dynamic function** or, more informally, **righ-hand-side** of a system <br /> of ordinary differential equations $\\dot{\\boldsymbol x} = \\boldsymbol f(t, \\boldsymbol x, \\boldsymbol u)$|\n",
    "| $\\boldsymbol x \\in \\mathbb{R}^{n} $ | An element of the **state space** of a controlled system of dimensionality $n$ |\n",
    "| $\\boldsymbol u \\in \\mathbb{R}^{m}$ | An element of the **action space** of a controlled system of dimensionality $m$ |\n",
    "| $\\boldsymbol y \\in \\mathbb{R}^{k}$ | An **observartion**|\n",
    "| $\\mathbb{X}\\subset \\mathbb{R}^{n} $| **State constraint set**|\n",
    "| $\\mathbb{U}\\subset \\mathbb{R}^{m} $| **Action constraint set**|\n",
    "| $\\boldsymbol h(\\cdot): \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{k}$ | **Observation function**  |\n",
    "| $\\rho(\\cdot) : \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ | **Policy** function |\n",
    "| $r(\\cdot) : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ | **Running cost** function  |\n",
    "\n",
    "\n",
    "### Goal\n",
    "Our main goal here is to implement the whole system almost from scratch and to apply Policy Gradient algorithm to [PID-regulator](https://en.wikipedia.org/wiki/PID_controller) coefficients (more precisely, P and D coefficient) tuning\n",
    "\n",
    "###  <font color=\"blue\"> Algorithm description </font>\n",
    "\n",
    "I. **Initialization**:\n",
    "- set iterations number **N_iterations**\n",
    "- set episodes number **N_episodes**\n",
    "- set **discount factor** $\\gamma$\n",
    "- initialize some **policy** parameters $\\theta_0$, learning rate $\\eta$\n",
    "\n",
    "II. **Main loop**:<br/>\n",
    "(Run episodical scenario)\n",
    ">**for** i in range(**N_iterations**):\n",
    ">>**for** j in range(**N_episodes**):\n",
    ">>> **while** **time** < **t1**:\n",
    "(corresponding utilized parts of Rcognita are provided in bold inside parentheses)\n",
    ">>>> - simulate environment evolution (**simulator**, **system**)\n",
    ">>>> - obtain observation (**system**) $\\boldsymbol y_i = \\boldsymbol h(\\boldsymbol x_i)$\n",
    ">>>> - obtain action (**actor**) $u_i \\sim  \\mathcal{N}(\\mu,\\,\\sigma^{2})$\n",
    ">>>> - compute and store new gradient (**actor.model**)\n",
    ">>>> - update accumulated outcome (**critic**): $\\sum_{i=0}^N \\gamma^i \\cdot\\left(y_i, u_i\\right)$\n",
    ">>> - compute and store REINFORCE objective gradient (**scenario**): $\\sum_{k=0}^N \\nabla_\\theta \\ln \\rho^\\theta(u_k \\vert y_k) \\cdot \\sum_{i=0}^N \\gamma^i \\cdot\\left(y_i, u_i\\right)$\n",
    ">> - compute mean overall stored REINFORCE objective gradients (**scenario**): $\\mathbb{E}\\left[\\sum_{k=0}^N \\nabla_\\theta \\ln \\rho^\\theta(u_k \\vert y_k) \\cdot \\sum_{i=0}^N \\gamma^i \\cdot\\left(y_i, u_i\\right)\\right]$\n",
    ">> - perform a gradient step (**scenario**): $\\theta_{i+1}=\\theta_i+\\eta \\mathbb{E}\\left[\\sum_{k=0}^N \\nabla_\\theta \\ln \\rho^\\theta(u_k \\vert y_k) \\cdot \\sum_{i=0}^N \\gamma^i \\cdot\\left(y_i, u_i\\right)\\right]$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25a5cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\"\"\"\n",
    "Just importing all the necessary stuff here.\n",
    "DO NOT CHANGE\n",
    "\"\"\"\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from rcognita_framework.pipelines.pipeline_inverted_pendulum import PipelineInvertedPendulum\n",
    "from rcognita_framework.rcognita.actors import ActorProbabilisticEpisodic\n",
    "from rcognita_framework.rcognita.critics import CriticTrivial\n",
    "from rcognita_framework.rcognita.systems import SysInvertedPendulum\n",
    "from rcognita_framework.rcognita.models import ModelGaussianConditional\n",
    "from rcognita_framework.rcognita.scenarios import EpisodicScenario\n",
    "from rcognita_framework.rcognita.utilities import rc\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039587ee",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\"> Section 1: System implementation </h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903acd6",
   "metadata": {},
   "source": [
    "<img style=\"left;\" src=\"n_pendulum.png\" width=18% height=18%>\n",
    "in our case the system has the following view\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "\\dot{\\varphi} = \\theta \\\\\n",
    "\\dot{\\theta} = \\frac{g}{l}\\sin{\\varphi} + \\frac{u}{ml^2} \\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Your task is to implement this system. More precisely, there are two crucial methods: \n",
    "* `_compute_state_dynamics(self, time, state, action, disturb)` - which computes and returns the $\\boldsymbol f(t, \\boldsymbol x, \\boldsymbol u)$ - the right-hand-side of the system. (You should fill the `Dstate` with correct values $(\\ddot\\boldsymbol \\varphi, \\dot{\\boldsymbol \\varphi})$)\n",
    "* `out(state, time, action)`- which yields us an observation $\\boldsymbol y = \\boldsymbol h(\\boldsymbol x)$, where $\\boldsymbol y = (\\varphi, \\int\\limits_0^t \\varphi dt, \\dot{\\varphi})$.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "509cbbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SysInvertedPendulumStudent(SysInvertedPendulum):\n",
    "    \"\"\"\n",
    "    System class: mathematical inverted pendulum\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def _compute_state_dynamics(self, time, state, action, disturb=[]):\n",
    "\n",
    "        m, g, l = self.pars[0], self.pars[1], self.pars[2]\n",
    "\n",
    "        Dstate = rc.zeros(self.dim_state, prototype=action)\n",
    "        Dstate[0] = state[1]\n",
    "        Dstate[1] = g / l * rc.sin(state[0]) + action[0] / (m * l ** 2)\n",
    "\n",
    "        return Dstate\n",
    "\n",
    "    def out(self, state, time=None, action=None):\n",
    "\n",
    "        # DEBUG ====================================\n",
    "        # observation = rc.zeros(self.dim_output)\n",
    "        # observation = state[:3] + measNoise  # <-- Measure only position and orientation\n",
    "        # observation = state  # <-- Position, force and torque sensors on\n",
    "        # if self.is_angle_overflow:\n",
    "        #     delta = np.abs(np.pi - state[0])\n",
    "        #     if state[0] > 0:\n",
    "        #         if state[0] > np.pi:\n",
    "        #             state = [-np.pi + delta, state[1]]\n",
    "        #     else:\n",
    "        #         if state[0] < -np.pi:\n",
    "        #             state = [np.pi - delta, state[1]]\n",
    "        # /DEBUG ===================================\n",
    "        delta_time = time - self.time_old\n",
    "        self.integral_alpha += delta_time * state[0]\n",
    "\n",
    "        return rc.array([state[0], self.integral_alpha, state[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26479e3a",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\"> Section 2: Conditional model implementation</h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc99856",
   "metadata": {},
   "source": [
    "In our setting we have a stochastic policy wich is modeled by a conditional distribution $\\rho^w_y(u) = \\frac{1}{\\sqrt{\\pi}}\\exp{-\\frac{(u-\\mu)^2}{0.5}}$,\n",
    "where $\\boldsymbol \\theta:=(\\theta_1, 0, \\theta_2)$, $\\mu:=-\\langle \\boldsymbol \\theta,\\boldsymbol y\\rangle$\n",
    "\n",
    "Implement the following methods:\n",
    "* `update_expectation(self, arg_condition)` - it should compute the expectation parameter of the distribution given the passed `arg_condition` \n",
    "* `compute_gradient(self, argin)`- self-explanatory :) Compute it yourself on the paper first.\n",
    "* `update(self)` - this method is being invoked after each gradient update. So it just basically resets the model. Note that you can access to `self.arg_condition_init` for these purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74bf77a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelGaussianConditionalStudent(ModelGaussianConditional):\n",
    "\n",
    "    model_name = \"model-gaussian\"\n",
    "\n",
    "    def update_expectation(self, arg_condition):\n",
    "        self.arg_condition = arg_condition\n",
    "        self.expectation = -rc.dot(arg_condition, self.weights)\n",
    "\n",
    "    def update_covariance(self):\n",
    "        self.covariance = 0.5\n",
    "\n",
    "    def compute_gradient(self, argin):\n",
    "        grad = (\n",
    "            -2 * self.arg_condition * (-argin[0] + self.expectation) / self.covariance\n",
    "        )\n",
    "        # grad = -self.arg_condition\n",
    "        return grad\n",
    "\n",
    "    def update(self, new_weights):\n",
    "        self.weights = np.clip(new_weights, 0, 100)\n",
    "        self.update_expectation(self.arg_condition_init)\n",
    "        self.update_covariance()\n",
    "\n",
    "    def sample_from_distribution(self, argin):\n",
    "        self.update_expectation(argin)\n",
    "        self.update_covariance()\n",
    "\n",
    "        return np.array([np.random.normal(self.expectation, self.covariance)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc70696b",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\"> Section 3: Actor implementation</h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf68765",
   "metadata": {},
   "source": [
    "As you remember from the introduction, actor is responsible for the action obtaining. During the episode simulation it samples action according to it's model. By default, `update(self, observation)` method performs this operation.\n",
    "But indeed we also should clip the action obtained from distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27dd5374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorProbabilisticEpisodicStudent(ActorProbabilisticEpisodic):\n",
    "\n",
    "    def update(self, observation):\n",
    "        action_sample = self.model.sample_from_distribution(observation)\n",
    "        self.action = np.array(\n",
    "            np.clip(action_sample, self.action_bounds[0], self.action_bounds[1])\n",
    "        )\n",
    "        self.action_old = self.action\n",
    "        current_gradient = self.model.compute_gradient(action_sample)\n",
    "        self.store_gradient(current_gradient)\n",
    "\n",
    "    def update_weights_by_gradient(self, gradient, learning_rate):\n",
    "        model_weights = self.model.weights\n",
    "        new_model_weights = rc.array(\n",
    "            model_weights - learning_rate * gradient * rc.array([1, 0, 1])\n",
    "        )\n",
    "\n",
    "        self.model.update(new_model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e9ed80",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\"> Section 4: Critic implementation</h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2f08857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticTrivialStudent(CriticTrivial):\n",
    "    \"\"\"\n",
    "    This is a dummy to calculate outcome (accumulated running objective).\n",
    "    Use an Euler method for that\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def update_outcome(self, observation, action):\n",
    "\n",
    "        self.outcome += self.running_objective(observation, action) * self.sampling_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c68ec2",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\"> Section 4: REINFORCE</h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e1d21d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicScenarioStudent(EpisodicScenario):\n",
    "\n",
    "    def REINFORCE_update(self):\n",
    "        self.outcomes_of_episodes.append(self.critic.outcome)\n",
    "        episode_REINFORCE_objective_gradient = self.critic.outcome * sum(\n",
    "            self.actor.gradients\n",
    "        )\n",
    "        self.episode_REINFORCE_objective_gradients.append(\n",
    "            episode_REINFORCE_objective_gradient\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b975d0",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#A7BD3F;\"> Section 5: Testing</h2>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "570d1fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineInvertedPendulumStudent(PipelineInvertedPendulum):\n",
    "\n",
    "    def initialize_system(self):\n",
    "        self.system = SysInvertedPendulumStudent(\n",
    "            sys_type=\"diff_eqn\",\n",
    "            dim_state=self.dim_state,\n",
    "            dim_input=self.dim_input,\n",
    "            dim_output=self.dim_output,\n",
    "            dim_disturb=self.dim_disturb,\n",
    "            pars=[self.m, self.g, self.l],\n",
    "            is_dynamic_controller=self.is_dynamic_controller,\n",
    "            is_disturb=self.is_disturb,\n",
    "            pars_disturb=[],\n",
    "        )\n",
    "        self.observation_init = self.system.out(self.state_init, time=0)\n",
    "\n",
    "    def initialize_models(self):\n",
    "        super().initialize_models()\n",
    "        self.actor_model = ModelGaussianConditionalStudent(\n",
    "            expectation_function=self.safe_controller,\n",
    "            arg_condition=self.observation_init,\n",
    "            weights=self.initial_weights,\n",
    "        )\n",
    "\n",
    "    def initialize_actor_critic(self):\n",
    "        self.critic = CriticTrivialStudent(\n",
    "            running_objective=self.running_objective, sampling_time=self.sampling_time\n",
    "        )\n",
    "        self.actor = ActorProbabilisticEpisodicStudent(\n",
    "            self.prediction_horizon,\n",
    "            self.dim_input,\n",
    "            self.dim_output,\n",
    "            self.control_mode,\n",
    "            self.action_bounds,\n",
    "            action_init=self.action_init,\n",
    "            predictor=self.predictor,\n",
    "            optimizer=self.actor_optimizer,\n",
    "            critic=self.critic,\n",
    "            running_objective=self.running_objective,\n",
    "            model=self.actor_model,\n",
    "        )\n",
    "\n",
    "    def initialize_scenario(self):\n",
    "        self.scenario = EpisodicScenarioStudent(\n",
    "            system=self.system,\n",
    "            simulator=self.simulator,\n",
    "            controller=self.controller,\n",
    "            actor=self.actor,\n",
    "            critic=self.critic,\n",
    "            logger=self.logger,\n",
    "            datafiles=self.datafiles,\n",
    "            time_final=self.time_final,\n",
    "            running_objective=self.running_objective,\n",
    "            no_print=self.no_print,\n",
    "            is_log=self.is_log,\n",
    "            is_playback=self.is_playback,\n",
    "            N_episodes=self.N_episodes,\n",
    "            N_iterations=self.N_iterations,\n",
    "            state_init=self.state_init,\n",
    "            action_init=self.action_init,\n",
    "        )\n",
    "\n",
    "    def execute_pipeline(self, **kwargs):\n",
    "        self.load_config()\n",
    "        self.setup_env()\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.initialize_system()\n",
    "        self.initialize_predictor()\n",
    "        self.initialize_safe_controller()\n",
    "        self.initialize_models()\n",
    "        self.initialize_objectives()\n",
    "        self.initialize_optimizers()\n",
    "        self.initialize_actor_critic()\n",
    "        self.initialize_controller()\n",
    "        self.initialize_simulator()\n",
    "        self.initialize_logger()\n",
    "        self.initialize_scenario()\n",
    "        if not self.no_visual and not self.save_trajectory:\n",
    "            self.initialize_visualizer()\n",
    "            self.main_loop_visual()\n",
    "        else:\n",
    "            self.scenario.run()\n",
    "            if self.is_playback:\n",
    "                self.playback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "109dd65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of simulation episode\n",
      "End of simulation episode\n",
      "End of simulation episode\n",
      "End of simulation episode\n",
      "End of simulation episode\n",
      "Your grade: 100.0\n"
     ]
    }
   ],
   "source": [
    "pipeline = PipelineInvertedPendulumStudent()\n",
    "pipeline.execute_pipeline(\n",
    "    no_visual=True, \n",
    "    t1=10, \n",
    "    is_playback=True, \n",
    "    N_episodes=1, \n",
    "    N_iterations=1, \n",
    "    no_print=True)\n",
    "\n",
    "mean_episodic = pipeline.scenario.outcome_episodic_mean[0]\n",
    "grade = np.clip(0.28 * mean_episodic + 200, 0, 100)\n",
    "print(f\"Your grade: {grade}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
