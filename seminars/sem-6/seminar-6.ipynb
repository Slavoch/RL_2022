{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d2a632",
   "metadata": {},
   "source": [
    "## Introduction to Rcognita for dynamical systems simulation\n",
    "\n",
    "* Config\n",
    "* Pipeline\n",
    "    * System\n",
    "    * Predictor\n",
    "    * Actor\n",
    "    * Critic\n",
    "    * Models\n",
    "    * Optimizers\n",
    "    * Controller\n",
    "    * Simulator\n",
    "    * Scenario\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db0cfd",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "pipelines.config_blueprints.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06430c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "from abc import abstractmethod, ABC\n",
    "from rcognita_framework.rcognita.utilities import rc\n",
    "from rcognita_framework.rcognita import (\n",
    "    systems, \n",
    "    predictors, \n",
    "    actors, \n",
    "    critics, \n",
    "    models, \n",
    "    optimizers, \n",
    "    controllers, \n",
    "    simulator, \n",
    "    scenarios,\n",
    "    animators\n",
    ")\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54d69d2",
   "metadata": {},
   "source": [
    "### Default argument parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6459d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadFromFile(argparse.Action):\n",
    "    def __call__(self, parser, namespace, values, option_string=None):\n",
    "        with values as f:\n",
    "            # parse arguments in the file and store them in the target namespace\n",
    "            parser.parse_args(f.read().split(), namespace)\n",
    "\n",
    "class RcognitaArgParser(argparse.ArgumentParser):\n",
    "    def __init__(self, description):\n",
    "\n",
    "        super().__init__(description=description)\n",
    "        self.add_argument(\n",
    "            \"--control_mode\",\n",
    "            metavar=\"control_mode\",\n",
    "            type=str,\n",
    "            choices=[\"manual\", \"nominal\", \"MPC\", \"RQL\", \"SQL\", \"STAG\"],\n",
    "            default=\"MPC\",\n",
    "            help=\"Control mode. Currently available: \"\n",
    "            + \"----manual: manual constant control specified by action_manual; \"\n",
    "            + \"----nominal: nominal controller, usually used to benchmark optimal controllers;\"\n",
    "            + \"----MPC:model-predictive control; \"\n",
    "            + \"----RQL: Q-learning actor-critic with prediction_horizon-1 roll-outs of stage objective; \"\n",
    "            + \"----SQL: stacked Q-learning; \"\n",
    "            + \"----STAG: joint actor-critic (stabilizing), system-specific, needs proper setup.\",\n",
    "        )\n",
    "        self.add_argument(\n",
    "            \"--is_log\",\n",
    "            action=\"store_true\",\n",
    "            help=\"Flag to log data into a data file. Data are stored in simdata folder.\",\n",
    "        )\n",
    "        self.add_argument(\n",
    "            \"--no_visual\",\n",
    "            action=\"store_true\",\n",
    "            help=\"Flag to produce graphical output.\",\n",
    "        )\n",
    "        self.add_argument(\n",
    "            \"--no_print\",\n",
    "            action=\"store_true\",\n",
    "            help=\"Flag to print simulation data into terminal.\",\n",
    "        )\n",
    "        self.add_argument(\n",
    "            \"--is_est_model\",\n",
    "            action=\"store_true\",\n",
    "            help=\"Flag to estimate environment model.\",\n",
    "        )\n",
    "        self.add_argument(\n",
    "            \"--save_trajectory\",\n",
    "            action=\"store_true\",\n",
    "            help=\"Flag to store trajectory inside the pipeline during execution.\",\n",
    "        )\n",
    "        self.add_argument(\n",
    "            \"--dt\",\n",
    "            type=float,\n",
    "            metavar=\"sampling_time\",\n",
    "            dest=\"sampling_time\",\n",
    "            default=0.01,\n",
    "            help=\"Controller sampling time.\",\n",
    "        )\n",
    "        self.add_argument(\n",
    "            \"--t1\",\n",
    "            type=float,\n",
    "            metavar=\"time_final\",\n",
    "            dest=\"time_final\",\n",
    "            default=10.0,\n",
    "            help=\"Final time of episode.\",\n",
    "        )\n",
    "        self.add_argument(\"--config\", type=open, action=LoadFromFile)\n",
    "        self.add_argument(\n",
    "            \"strings\", metavar=\"STRING\", nargs=\"*\", help=\"String for searching\",\n",
    "        )\n",
    "\n",
    "        self.add_argument(\n",
    "            \"-f\",\n",
    "            \"--file\",\n",
    "            help=\"Path for input file. First line should contain number of lines to search in\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05cdc1d",
   "metadata": {},
   "source": [
    "### Config metaclass\n",
    "\n",
    "What it basically does is collecting of arguments parsed from sys.argv and save into local namespace afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ed8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaConf(type):\n",
    "    def __init__(cls, name, bases, clsdict):\n",
    "        if \"argument_parser\" in clsdict:\n",
    "\n",
    "            def new_argument_parser(self):\n",
    "                args = clsdict[\"argument_parser\"](self)\n",
    "                self.__dict__.update(vars(args))\n",
    "\n",
    "            setattr(cls, \"argument_parser\", new_argument_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c01a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractConfig(object, metaclass=MetaConf):\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        self.config_name = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def argument_parser(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def pre_processing(self):\n",
    "        pass\n",
    "\n",
    "    def get_env(self):\n",
    "        self.argument_parser()\n",
    "        self.pre_processing()\n",
    "        return self.__dict__\n",
    "    \n",
    "    def config_to_pickle(self):\n",
    "        with open(\n",
    "            f\"../tests/refs/env_{self.config_name}.pickle\", \"wb\"\n",
    "        ) as env_description_out:\n",
    "            pickle.dump(self.__dict__, env_description_out) ### Dump environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb020a0f",
   "metadata": {},
   "source": [
    "### Config for 3-wheel-robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f64b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config3WRobotNI(AbstractConfig):\n",
    "    def __init__(self):\n",
    "        self.config_name = \"3wrobot_NI\"\n",
    "\n",
    "    def argument_parser(self):\n",
    "        description = \"Agent-environment pipeline: a 3-wheel robot (kinematic model a. k. a. non-holonomic integrator).\"\n",
    "\n",
    "        parser = RcognitaArgParser(description=description)\n",
    "\n",
    "        parser.add_argument(\n",
    "            \"--Nruns\",\n",
    "            type=int,\n",
    "            default=1,\n",
    "            help=\"Number of episodes. Learned parameters are not reset after an episode.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--state_init\",\n",
    "            type=str,\n",
    "            nargs=\"+\",\n",
    "            metavar=\"state_init\",\n",
    "            default=[\"5\", \"5\", \"-3*pi/4\"],\n",
    "            help=\"Initial state (as sequence of numbers); \"\n",
    "            + \"dimension is environment-specific!\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--model_est_stage\",\n",
    "            type=float,\n",
    "            default=1.0,\n",
    "            help=\"Seconds to learn model until benchmarking controller kicks in.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--model_est_period_multiplier\",\n",
    "            type=float,\n",
    "            default=1,\n",
    "            help=\"Model is updated every model_est_period_multiplier times sampling_time seconds.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--model_order\",\n",
    "            type=int,\n",
    "            default=5,\n",
    "            help=\"Order of state-space estimation model.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--prob_noise_pow\",\n",
    "            type=float,\n",
    "            default=False,\n",
    "            help=\"Power of probing (exploration) noise.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--action_manual\",\n",
    "            type=float,\n",
    "            default=[-5, -3],\n",
    "            nargs=\"+\",\n",
    "            help=\"Manual control action to be fed constant, system-specific!\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--prediction_horizon\",\n",
    "            type=int,\n",
    "            default=3,\n",
    "            help=\"Horizon length (in steps) for predictive controllers.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--pred_step_size_multiplier\",\n",
    "            type=float,\n",
    "            default=1.0,\n",
    "            help=\"Size of each prediction step in seconds is a pred_step_size_multiplier multiple of controller sampling time sampling_time.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--data_buffer_size\",\n",
    "            type=int,\n",
    "            default=10,\n",
    "            help=\"Size of the buffer (experience replay) for model estimation, agent learning etc.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--running_obj_struct\",\n",
    "            type=str,\n",
    "            default=\"quadratic\",\n",
    "            choices=[\"quadratic\", \"biquadratic\"],\n",
    "            help=\"Structure of stage objective function.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--R1_diag\",\n",
    "            type=float,\n",
    "            nargs=\"+\",\n",
    "            default=[1, 10, 1, 0, 0],\n",
    "            help=\"Parameter of stage objective function. Must have proper dimension. \"\n",
    "            + \"Say, if chi = [observation, action], then a quadratic stage objective reads chi.T diag(R1) chi, where diag() is transformation of a vector to a diagonal matrix.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--R2_diag\",\n",
    "            type=float,\n",
    "            nargs=\"+\",\n",
    "            default=[1, 10, 1, 0, 0],\n",
    "            help=\"Parameter of stage objective function . Must have proper dimension. \"\n",
    "            + \"Say, if chi = [observation, action], then a bi-quadratic stage objective reads chi**2.T diag(R2) chi**2 + chi.T diag(R1) chi, \"\n",
    "            + \"where diag() is transformation of a vector to a diagonal matrix.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--discount_factor\", type=float, default=1.0, help=\"Discount factor.\"\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--critic_period_multiplier\",\n",
    "            type=float,\n",
    "            default=1.0,\n",
    "            help=\"Critic is updated every critic_period_multiplier times sampling_time seconds.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--critic_struct\",\n",
    "            type=str,\n",
    "            default=\"quad-nomix\",\n",
    "            choices=[\"quad-lin\", \"quadratic\", \"quad-nomix\", \"quad-mix\", \"NN\"],\n",
    "            help=\"Feature structure (critic). Currently available: \"\n",
    "            + \"----quad-lin: quadratic-linear; \"\n",
    "            + \"----quadratic: quadratic; \"\n",
    "            + \"----quad-nomix: quadratic, no mixed terms; \"\n",
    "            + \"----quad-mix: quadratic, mixed observation-action terms (for, say, action_objective or advantage function approximations).\"\n",
    "            + \"----NN: Torch neural network.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--actor_struct\",\n",
    "            type=str,\n",
    "            default=\"container\",\n",
    "            choices=[\"quad-lin\", \"quadratic\", \"quad-nomix\", \"container\"],\n",
    "            help=\"Feature structure (actor). Currently available: \"\n",
    "            + \"----quad-lin: quadratic-linear; \"\n",
    "            + \"----quadratic: quadratic; \"\n",
    "            + \"----quad-nomix: quadratic, no mixed terms.\"\n",
    "            + \"----container: container, just to store actions\",\n",
    "        )\n",
    "\n",
    "        args = parser.parse_args()\n",
    "        return args\n",
    "\n",
    "    def pre_processing(self):\n",
    "        self.trajectory = []\n",
    "        self.dim_state = 3\n",
    "        self.dim_input = 2\n",
    "        self.dim_output = self.dim_state\n",
    "        self.dim_disturb = 2\n",
    "        if self.control_mode == \"STAG\":\n",
    "            self.prediction_horizon = 1\n",
    "\n",
    "        self.dim_R1 = self.dim_output + self.dim_input\n",
    "        self.dim_R2 = self.dim_R1\n",
    "        # ----------------------------------------Post-processing of arguments\n",
    "        # Convert `pi` to a number pi\n",
    "        for k in range(len(self.state_init)):\n",
    "            self.state_init[k] = eval(self.state_init[k].replace(\"pi\", str(np.pi)))\n",
    "\n",
    "        self.state_init = np.array(self.state_init)\n",
    "        self.action_manual = np.array(self.action_manual)\n",
    "\n",
    "        self.pred_step_size = self.sampling_time * self.pred_step_size_multiplier\n",
    "        self.model_est_period = self.sampling_time * self.model_est_period_multiplier\n",
    "        self.critic_period = self.sampling_time * self.critic_period_multiplier\n",
    "\n",
    "        self.R1 = np.diag(np.array(self.R1_diag))\n",
    "        self.R2 = np.diag(np.array(self.R2_diag))\n",
    "\n",
    "        assert self.time_final > self.sampling_time > 0.0\n",
    "        assert self.state_init.size == self.dim_state\n",
    "\n",
    "        # ----------------------------------------(So far) fixed settings\n",
    "        self.is_disturb = 0\n",
    "        self.is_dynamic_controller = 0\n",
    "\n",
    "        self.time_start = 0\n",
    "\n",
    "        self.action_init = 0 * np.ones(self.dim_input)\n",
    "\n",
    "        # Solver\n",
    "        self.atol = 1e-5\n",
    "        self.rtol = 1e-3\n",
    "\n",
    "        # xy-plane\n",
    "        self.xMin = -10\n",
    "        self.xMax = 10\n",
    "        self.yMin = -10\n",
    "        self.yMax = 10\n",
    "\n",
    "        # Model estimator stores models in a stack and recall the best of model_est_checks\n",
    "        self.model_est_checks = 0\n",
    "\n",
    "        # Control constraints\n",
    "        self.v_min = -25\n",
    "        self.v_max = 25\n",
    "        self.omega_min = -5\n",
    "        self.omega_max = 5\n",
    "        self.action_bounds = np.array(\n",
    "            [[self.v_min, self.v_max], [self.omega_min, self.omega_max]]\n",
    "        )\n",
    "\n",
    "        self.xCoord0 = self.state_init[0]\n",
    "        self.yCoord0 = self.state_init[1]\n",
    "        self.angle0 = self.state_init[2]\n",
    "        self.angle_deg_0 = self.angle0 / 2 / np.pi\n",
    "        self.observation_target = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5edf51",
   "metadata": {},
   "source": [
    "### Pipeline: System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05065a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sys3WRobotNISeminar(systems.System):\n",
    "    \"\"\"\n",
    "    System class: 3-wheel robot with static actuators (the NI - non-holonomic integrator).\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.name = \"3wrobotNI\"\n",
    "\n",
    "        if self.is_disturb:\n",
    "            self.sigma_disturb = self.pars_disturb[0]\n",
    "            self.mu_disturb = self.pars_disturb[1]\n",
    "            self.tau_disturb = self.pars_disturb[2]\n",
    "\n",
    "    def _compute_state_dynamics(self, time, state, action, disturb=[]):\n",
    "\n",
    "        Dstate = rc.zeros(self.dim_state)\n",
    "\n",
    "        if self.is_disturb and (disturb != []):\n",
    "            Dstate[0] = action[0] * rc.cos(state[2]) + disturb[0]\n",
    "            Dstate[1] = action[0] * rc.sin(state[2]) + disturb[0]\n",
    "            Dstate[2] = action[1] + disturb[1]\n",
    "        else:\n",
    "            Dstate[0] = action[0] * rc.cos(state[2])\n",
    "            Dstate[1] = action[0] * rc.sin(state[2])\n",
    "            Dstate[2] = action[1]\n",
    "\n",
    "        return Dstate\n",
    "\n",
    "    def out(self, state, time=None, action=None):\n",
    "\n",
    "        return state\n",
    "    \n",
    "    \n",
    "    def compute_closed_loop_rhs(self, time, state_full):\n",
    "        rhs_full_state = rc.zeros(self._dim_full_state, prototype=state_full)\n",
    "\n",
    "        state = state_full[0 : self.dim_state]\n",
    "\n",
    "        if self.is_disturb:\n",
    "            disturb = state_full[self.dim_state :]\n",
    "        else:\n",
    "            disturb = []\n",
    "\n",
    "        if self.is_dynamic_controller:\n",
    "            action = state_full[-self.dim_input :]\n",
    "            observation = self.out(state)\n",
    "            rhs_full_state[-self.dim_input :] = self._ctrlDyn(time, action, observation)\n",
    "        else:\n",
    "            # Fetch the control action stored in the system\n",
    "            action = self.action\n",
    "\n",
    "        rhs_full_state[0 : self.dim_state] = self._compute_state_dynamics(\n",
    "            time, state, action, disturb\n",
    "        ).T\n",
    "\n",
    "        if self.is_disturb:\n",
    "            rhs_full_state[self.dim_state :] = self._compute_disturbance_dynamics(\n",
    "                time, disturb\n",
    "            ).T\n",
    "\n",
    "        # Track system's state\n",
    "        self._state = state\n",
    "\n",
    "        return rhs_full_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2e974c",
   "metadata": {},
   "source": [
    "### Pipeline: Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536037f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EulerPredictorSeminar(predictors.EulerPredictor):\n",
    "    \"\"\"\n",
    "    Euler predictor uses a simple Euler discretization scheme.\n",
    "    It does predictions by increments \n",
    "    scaled by a sampling time times the velocity evaluated at each successive node.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def predict(self, current_state_or_observation, action):\n",
    "        next_state_or_observation = (\n",
    "            current_state_or_observation\n",
    "            + self.pred_step_size\n",
    "            * self.compute_state_dynamics([], current_state_or_observation, action)\n",
    "        )\n",
    "        return next_state_or_observation\n",
    "\n",
    "    def predict_sequence(self, observation, action_sequence):\n",
    "\n",
    "        observation_sequence = rc.zeros(\n",
    "            [self.prediction_horizon, self.dim_output], prototype=action_sequence\n",
    "        )\n",
    "        current_observation = observation\n",
    "\n",
    "        for k in range(self.prediction_horizon):\n",
    "            current_action = action_sequence[k, :]\n",
    "            next_observation = self.predict(current_observation, current_action)\n",
    "            observation_sequence[k, :] = self.sys_out(next_observation)\n",
    "            current_observation = next_observation\n",
    "        return observation_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baac933",
   "metadata": {},
   "source": [
    "### Pipeline: Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e2955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SciPyOptimizer(optimizers.BaseOptimizer):\n",
    "    engine = \"SciPy\"\n",
    "\n",
    "    def __init__(self, opt_method, opt_options, verbose=True):\n",
    "        self.opt_method = opt_method\n",
    "        self.opt_options = opt_options\n",
    "        self.verbose = verbose\n",
    "\n",
    "    @optimizers.BaseOptimizer.verbose\n",
    "    def optimize(self, objective, initial_guess, bounds, constraints=(), verbose=True):\n",
    "\n",
    "        weight_bounds = sp.optimize.Bounds(bounds[0], bounds[1], keep_feasible=True)\n",
    "\n",
    "        before_opt = objective(initial_guess)\n",
    "        opt_result = minimize(\n",
    "            objective,\n",
    "            x0=initial_guess,\n",
    "            method=self.opt_method,\n",
    "            bounds=weight_bounds,\n",
    "            options=self.opt_options,\n",
    "            constraints=constraints,\n",
    "            tol=1e-7,\n",
    "        )\n",
    "        if verbose:\n",
    "            print(f\"before:{before_opt},\\nafter:{opt_result.fun}\")\n",
    "\n",
    "        return opt_result.x\n",
    "\n",
    "\n",
    "class CasADiOptimizer(optimizers.BaseOptimizer):\n",
    "    engine = \"CasADi\"\n",
    "\n",
    "    def __init__(self, opt_method, opt_options, verbose=True):\n",
    "        self.opt_method = opt_method\n",
    "        self.opt_options = opt_options\n",
    "        self.verbose = verbose\n",
    "\n",
    "    @optimizers.BaseOptimizer.verbose\n",
    "    def optimize(\n",
    "        self,\n",
    "        objective,\n",
    "        initial_guess,\n",
    "        bounds,\n",
    "        constraints=(),\n",
    "        decision_variable_symbolic=None,\n",
    "    ):\n",
    "        optimization_problem = {\n",
    "            \"f\": objective,\n",
    "            \"x\": rc.concatenate(decision_variable_symbolic),\n",
    "            \"g\": (),\n",
    "        }\n",
    "\n",
    "        if isinstance(constraints, tuple):\n",
    "            upper_bound_constraint = [0 for _ in constraints]\n",
    "        elif isinstance(constraints, (SX, DM, int, float)):\n",
    "            upper_bound_constraint = [0]\n",
    "\n",
    "        try:\n",
    "            solver = nlpsol(\n",
    "                \"solver\", self.opt_method, optimization_problem, self.opt_options,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return initial_guess\n",
    "\n",
    "        if upper_bound_constraint is not None and len(upper_bound_constraint) > 0:\n",
    "            result = solver(\n",
    "                x0=initial_guess,\n",
    "                lbx=bounds[0],\n",
    "                ubx=bounds[1],\n",
    "                ubg=upper_bound_constraint,\n",
    "            )\n",
    "        else:\n",
    "            result = solver(x0=initial_guess, lbx=bounds[0], ubx=bounds[1])\n",
    "\n",
    "        return result[\"x\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc16e302",
   "metadata": {},
   "source": [
    "### Pipeline: Actor\n",
    "Main parts of actor are:\n",
    "* `objective`\n",
    "* `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11261d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorSeminar(actors.Actor):\n",
    "\n",
    "    def objective(\n",
    "        self, action_sequence, observation,\n",
    "    ):\n",
    "        action_sequence_reshaped = rc.reshape(\n",
    "            action_sequence, [self.prediction_horizon + 1, self.dim_input]\n",
    "        )\n",
    "\n",
    "        observation_sequence = [observation]\n",
    "\n",
    "        observation_sequence_predicted = self.predictor.predict_sequence(\n",
    "            observation, action_sequence_reshaped\n",
    "        )\n",
    "\n",
    "        observation_cur = rc.reshape(observation, [1, self.dim_output])\n",
    "\n",
    "        observation_sequence = rc.vstack(\n",
    "            (observation_cur, observation_sequence_predicted)\n",
    "        )\n",
    "\n",
    "        actor_objective = 0\n",
    "        for k in range(self.prediction_horizon):\n",
    "            actor_objective += self.discount_factor ** k * self.running_objective(\n",
    "                observation_sequence[k, :].T, action_sequence_reshaped[k, :].T\n",
    "            )\n",
    "        return actor_objective\n",
    "\n",
    "    def update(self, observation, constraint_functions=(), time=None):\n",
    "        \"\"\"\n",
    "        Method to update the current action or weight tensor.\n",
    "        The old (previous) action or weight tensor is stored inside model istself.\n",
    "        The `time` argument is used for debugging purposes.\n",
    "        \"\"\"\n",
    "\n",
    "        action_sequence_old = rc.rep_mat(\n",
    "            self.model.cache.weights, 1, self.prediction_horizon + 1\n",
    "        )\n",
    "\n",
    "        action_sequence_init_reshaped = rc.reshape(\n",
    "            action_sequence_old, [(self.prediction_horizon + 1) * self.dim_input,],\n",
    "        )\n",
    "\n",
    "        constraints = ()\n",
    "\n",
    "        if self.optimizer.engine == \"CasADi\":\n",
    "            action_sequence_init_reshaped = rc.DM(action_sequence_init_reshaped)\n",
    "\n",
    "            symbolic_dummy = rc.array_symb((1, 1))\n",
    "\n",
    "            symbolic_var = rc.array_symb(\n",
    "                tup=rc.shape(action_sequence_init_reshaped), prototype=symbolic_dummy\n",
    "            )\n",
    "\n",
    "            actor_objective = lambda action_sequence: self.objective(\n",
    "                action_sequence, observation\n",
    "            )\n",
    "\n",
    "            actor_objective = rc.lambda2symb(actor_objective, symbolic_var)\n",
    "\n",
    "            if constraint_functions:\n",
    "                constraints = self.create_constraints(\n",
    "                    constraint_functions, symbolic_var, observation\n",
    "                )\n",
    "\n",
    "            action_sequence_optimized = self.optimizer.optimize(\n",
    "                actor_objective,\n",
    "                action_sequence_init_reshaped,\n",
    "                self.action_bounds,\n",
    "                constraints=constraints,\n",
    "                decision_variable_symbolic=symbolic_var,\n",
    "            )\n",
    "\n",
    "        elif self.optimizer.engine == \"SciPy\":\n",
    "            actor_objective = rc.func_to_lambda_with_params(\n",
    "                self.objective, observation, var_prototype=action_sequence_init_reshaped\n",
    "            )\n",
    "\n",
    "            if constraint_functions:\n",
    "                constraints = sp.optimize.NonlinearConstraint(\n",
    "                    partial(\n",
    "                        self.create_constraints,\n",
    "                        constraint_functions=constraint_functions,\n",
    "                        observation=observation,\n",
    "                    ),\n",
    "                    -np.inf,\n",
    "                    0,\n",
    "                )\n",
    "\n",
    "            action_sequence_optimized = self.optimizer.optimize(\n",
    "                actor_objective,\n",
    "                action_sequence_init_reshaped,\n",
    "                self.action_bounds,\n",
    "                constraints=constraints,\n",
    "            )\n",
    "\n",
    "        \n",
    "        action_obtained = action_sequence_optimized[: self.dim_input]\n",
    "        self.model.update_and_cache_weights(action_obtained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb3a5b1",
   "metadata": {},
   "source": [
    "### Pipeline: Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76b94d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticTrivialSeminar(critics.Critic):\n",
    "    \"\"\"\n",
    "    This is a dummy to calculate outcome (accumulated running objective).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, running_objective, sampling_time=0.01):\n",
    "        self.running_objective = running_objective\n",
    "        self.sampling_time = sampling_time\n",
    "        self.outcome = 0\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.outcome\n",
    "\n",
    "    def objective(self, weights):\n",
    "        pass\n",
    "\n",
    "    def get_optimized_weights(self, constraint_functions=(), time=None):\n",
    "        pass\n",
    "\n",
    "    def update_buffers(self, observation, action):\n",
    "        self.update_outcome(observation, action)\n",
    "\n",
    "    def update(self, constraint_functions=(), time=None):\n",
    "        pass\n",
    "\n",
    "    def update_outcome(self, observation, action):\n",
    "\n",
    "        self.outcome += self.running_objective(observation, action) * self.sampling_time\n",
    "\n",
    "    def reset(self):\n",
    "        self.outcome = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11accc25",
   "metadata": {},
   "source": [
    "### Pipeline: Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e106718",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelAbstract(ABC):\n",
    "    \"\"\"\n",
    "    Blueprint of a model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, *args, weights=None, use_stored_weights=False):\n",
    "\n",
    "        if use_stored_weights is False:\n",
    "            if weights is not None:\n",
    "                return self.forward(*args, weights=weights)\n",
    "            else:\n",
    "                return self.forward(*args, weights=self.weights)\n",
    "        else:\n",
    "            return self.cache.forward(*args, weights=self.weights)\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def model_name(self):\n",
    "        return \"model_name\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    def update_and_cache_weights(self, weights):\n",
    "        if \"cache\" not in self.__dict__.keys():\n",
    "            self.cache = deepcopy(self)\n",
    "\n",
    "        self.weights = weights\n",
    "\n",
    "        self.cache.weights = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38552016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWeightContainerSeminar(ModelAbstract):\n",
    "    \"\"\"\n",
    "    A special model which just basically stores weights\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model_name = \"action-sequence\"\n",
    "\n",
    "    def __init__(self, weights_init=None):\n",
    "        self.weights = weights_init\n",
    "        self.update_and_cache_weights(self.weights)\n",
    "\n",
    "    def forward(self, *argin):\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd057571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelQuadNoMix(ModelAbstract):\n",
    "    \"\"\"\n",
    "    Quadratic model (no mixed terms).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model_name = \"quad-nomix\"\n",
    "\n",
    "    def __init__(self, input_dim, sinle_weight_min=1.0, sinle_weight_max=1e3):\n",
    "        self.dim_weights = input_dim\n",
    "        self.weight_min = sinle_weight_min * np.ones(self.dim_weights)\n",
    "        self.weight_max = sinle_weight_max * np.ones(self.dim_weights)\n",
    "        self.weights = self.weight_min\n",
    "        self.update_and_cache_weights(self.weights)\n",
    "\n",
    "    def forward(self, *argin, weights):\n",
    "        if len(argin) > 1:\n",
    "            vec = rc.concatenate(tuple(argin))\n",
    "        else:\n",
    "            vec = argin[0]\n",
    "\n",
    "        polynom = vec * vec\n",
    "        result = rc.dot(weights, polynom)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c8018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelQuadForm(ModelAbstract):\n",
    "    \"\"\"\n",
    "    Quadratic form.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model_name = \"quad_form\"\n",
    "\n",
    "    def __init__(self, weights=None):\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, *argin, weights):\n",
    "\n",
    "        if len(argin) != 2:\n",
    "            raise ValueError(\"ModelQuadForm assumes two vector arguments!\")\n",
    "\n",
    "        vec = rc.concatenate(tuple(argin))\n",
    "\n",
    "        result = vec.T @ weights @ vec\n",
    "\n",
    "        result = rc.squeeze(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb02b61",
   "metadata": {},
   "source": [
    "### Pipeline: Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5702c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalController(ABC):\n",
    "    \"\"\"\n",
    "    A blueprint of optimal controllers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, time_start=0, sampling_time=0.1, observation_target=[],\n",
    "    ):\n",
    "\n",
    "        self.controller_clock = time_start\n",
    "        self.sampling_time = sampling_time\n",
    "\n",
    "        self.observation_target = observation_target\n",
    "\n",
    "    def estimate_model(self, observation, time):\n",
    "        if self.is_est_model or self.mode in [\"RQL\", \"SQL\"]:\n",
    "            self.estimator.estimate_model(observation, time)\n",
    "\n",
    "    def reset(self, time_start):\n",
    "        \"\"\"\n",
    "        Resets agent for use in multi-episode simulation.\n",
    "        Only internal clock and current actions are reset.\n",
    "        All the learned parameters are retained.\n",
    "\n",
    "        \"\"\"\n",
    "        self.controller_clock = time_start\n",
    "        self.actor.model.weights = self.actor.action_init\n",
    "        self.actor.model.update_and_cache_weights(self.actor.action_init)\n",
    "\n",
    "    def compute_action_sampled(self, time, observation, constraints=()):\n",
    "\n",
    "        time_in_sample = time - self.controller_clock\n",
    "        timeInCriticPeriod = time - self.critic_clock\n",
    "        is_critic_update = timeInCriticPeriod >= self.critic_period\n",
    "\n",
    "        # CRITIC CLOKC NOT UPDATED!!!! FIX !!!!\n",
    "\n",
    "        if time_in_sample >= self.sampling_time:  # New sample\n",
    "            # Update controller's internal clock\n",
    "            self.controller_clock = time\n",
    "\n",
    "            action = self.compute_action(\n",
    "                time, observation, is_critic_update=is_critic_update\n",
    "            )\n",
    "\n",
    "            return action\n",
    "\n",
    "        else:\n",
    "            return self.actor.model.cache.weights\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_action(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class RLControllerSeminar(OptimalController):\n",
    "    \"\"\"\n",
    "    Reinforcement learning controller class.\n",
    "    Takes instances of `actor` and `critic` to operate.\n",
    "    Action computation is sampled, i.e., actions are computed at discrete, equi-distant moments in time.\n",
    "    `critic` in turn is updated every `critic_period` units of time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, *args, critic_period=0.1, actor=[], critic=[], time_start=0, **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "\n",
    "        self.dim_input = self.actor.dim_input\n",
    "        self.dim_output = self.actor.dim_output\n",
    "\n",
    "        self.critic_clock = time_start\n",
    "        self.critic_period = critic_period\n",
    "\n",
    "    def compute_action(\n",
    "        self, time, observation, is_critic_update=False,\n",
    "    ):\n",
    "        # Critic\n",
    "\n",
    "        # Update data buffers\n",
    "        self.critic.update_buffers(observation, self.actor.model.cache.weights)\n",
    "\n",
    "        if is_critic_update:\n",
    "            # Update critic's internal clock\n",
    "            self.critic_clock = time\n",
    "            self.critic.update(time=time)\n",
    "\n",
    "        self.actor.update(observation)\n",
    "        action = self.actor.get_action()\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d615b1",
   "metadata": {},
   "source": [
    "### Pipeline: Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16d988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineScenarioSeminar:\n",
    "    \"\"\"\n",
    "    Online scenario: the controller and system interact with each other live via exchange of observations and actions, successively in time steps.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        system,\n",
    "        simulator,\n",
    "        controller,\n",
    "        actor,\n",
    "        critic,\n",
    "        logger,\n",
    "        datafiles,\n",
    "        time_final,\n",
    "        running_objective,\n",
    "        no_print=False,\n",
    "        is_log=False,\n",
    "        is_playback=False,\n",
    "        state_init=None,\n",
    "        action_init=None,\n",
    "    ):\n",
    "        self.system = system\n",
    "        self.simulator = simulator\n",
    "        self.controller = controller\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.logger = logger\n",
    "        self.time_final = time_final\n",
    "        self.outcome = 0\n",
    "        self.datafile = datafiles[0]\n",
    "        self.running_objective = running_objective\n",
    "        self.trajectory = []\n",
    "        self.no_print = no_print\n",
    "        self.is_log = is_log\n",
    "        self.time_old = 0\n",
    "        self.is_playback = is_playback\n",
    "        self.state_init = state_init\n",
    "        self.action_init = action_init\n",
    "        if self.is_playback:\n",
    "            self.table = []\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            self.step()\n",
    "\n",
    "    def step(self):\n",
    "        sim_status = self.simulator.do_sim_step()\n",
    "        if sim_status == -1:\n",
    "            return -1\n",
    "\n",
    "        (\n",
    "            self.time,\n",
    "            _,\n",
    "            self.observation,\n",
    "            self.state_full,\n",
    "        ) = self.simulator.get_sim_step_data()\n",
    "        self.trajectory.append(rc.concatenate((self.state_full, self.time), axis=None))\n",
    "        delta_time = self.time - self.time_old\n",
    "        self.time_old = self.time\n",
    "\n",
    "        self.action = self.controller.compute_action_sampled(\n",
    "            self.time, self.observation\n",
    "        )\n",
    "        self.system.receive_action(self.action)\n",
    "\n",
    "        self.running_objective_value = self.running_objective(\n",
    "            self.observation, self.action\n",
    "        )\n",
    "        self.update_outcome(self.observation, self.action, delta_time)\n",
    "\n",
    "        if not self.no_print:\n",
    "            self.logger.print_sim_step(\n",
    "                self.time,\n",
    "                self.state_full,\n",
    "                self.action,\n",
    "                self.running_objective_value,\n",
    "                self.outcome,\n",
    "            )\n",
    "        if self.is_log:\n",
    "            self.logger.log_data_row(\n",
    "                self.datafile,\n",
    "                self.time,\n",
    "                self.state_full,\n",
    "                self.action,\n",
    "                self.running_objective_value,\n",
    "                self.outcome,\n",
    "            )\n",
    "\n",
    "        if self.is_playback:\n",
    "            self.table.append(\n",
    "                [\n",
    "                    self.time,\n",
    "                    *self.state_full,\n",
    "                    *self.action,\n",
    "                    self.running_objective_value,\n",
    "                    self.outcome,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def update_outcome(self, observation, action, delta):\n",
    "\n",
    "        \"\"\"\n",
    "        Sample-to-sample accumulated (summed up or integrated) stage objective. This can be handy to evaluate the performance of the agent.\n",
    "        If the agent succeeded to stabilize the system, ``outcome`` would converge to a finite value which is the performance mark.\n",
    "        The smaller, the better (depends on the problem specification of course - you might want to maximize objective instead).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.outcome += self.running_objective(observation, action) * delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa16de",
   "metadata": {},
   "source": [
    "# Test pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7110b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rcognita_framework.pipelines.pipeline_3wrobot_NI import Pipeline3WRobotNI\n",
    "\n",
    "class Pipeline3WRobotNISeminar(Pipeline3WRobotNI):\n",
    "    config = Config3WRobotNI\n",
    "\n",
    "    def initialize_system(self):\n",
    "        self.system = Sys3WRobotNISeminar(\n",
    "            sys_type=\"diff_eqn\",\n",
    "            dim_state=self.dim_state,\n",
    "            dim_input=self.dim_input,\n",
    "            dim_output=self.dim_output,\n",
    "            dim_disturb=self.dim_disturb,\n",
    "            pars=[],\n",
    "            is_dynamic_controller=self.is_dynamic_controller,\n",
    "            is_disturb=self.is_disturb,\n",
    "            pars_disturb=rc.array(\n",
    "                [\n",
    "                    [200 * self.sampling_time, 200 * self.sampling_time],\n",
    "                    [0, 0],\n",
    "                    [0.3, 0.3],\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "    def initialize_predictor(self):\n",
    "        self.predictor = EulerPredictorSeminar(\n",
    "            self.pred_step_size,\n",
    "            self.system._compute_state_dynamics,\n",
    "            self.system.out,\n",
    "            self.dim_output,\n",
    "            self.prediction_horizon,\n",
    "        )\n",
    "        \n",
    "    def initialize_models(self):\n",
    "        self.dim_critic_model_input = self.dim_input + self.dim_output\n",
    "        self.critic_model = ModelQuadNoMix(self.dim_critic_model_input)\n",
    "        self.actor_model = models.ModelWeightContainer(weights_init=self.action_init)\n",
    "        self.model_running_objective = ModelQuadForm(weights=self.R1)\n",
    "\n",
    "        A = rc.zeros([self.model_order, self.model_order])\n",
    "        B = rc.zeros([self.model_order, self.dim_input])\n",
    "        C = rc.zeros([self.dim_output, self.model_order])\n",
    "        D = rc.zeros([self.dim_output, self.dim_input])\n",
    "        initial_guessest = rc.zeros(self.model_order)\n",
    "\n",
    "        self.model_SS = models.ModelSS(A, B, C, D, initial_guessest)\n",
    "        \n",
    "    def initialize_optimizers(self):\n",
    "        opt_options = {\n",
    "            \"maxiter\": 500,\n",
    "            \"maxfev\": 5000,\n",
    "            \"disp\": False,\n",
    "            \"adaptive\": True,\n",
    "            \"xatol\": 1e-7,\n",
    "            \"fatol\": 1e-7,\n",
    "        }\n",
    "        self.actor_optimizer = optimizers.SciPyOptimizer(\n",
    "            opt_method=\"SLSQP\", opt_options=opt_options\n",
    "        )\n",
    "        self.critic_optimizer = optimizers.SciPyOptimizer(\n",
    "            opt_method=\"SLSQP\", opt_options=opt_options,\n",
    "        )\n",
    "    \n",
    "    def initialize_actor_critic(self):\n",
    "        self.critic = CriticTrivialSeminar(self.running_objective, self.sampling_time)\n",
    "\n",
    "        self.actor = ActorSeminar(\n",
    "            self.prediction_horizon,\n",
    "            self.dim_input,\n",
    "            self.dim_output,\n",
    "            self.control_mode,\n",
    "            self.action_bounds,\n",
    "            action_init=self.action_init,\n",
    "            predictor=self.predictor,\n",
    "            optimizer=self.actor_optimizer,\n",
    "            critic=self.critic,\n",
    "            running_objective=self.running_objective,\n",
    "            model=self.actor_model,\n",
    "        )\n",
    "\n",
    "\n",
    "    def initialize_visualizer(self):\n",
    "\n",
    "        self.animator = animators.Animator3WRobotNI(\n",
    "            objects=(\n",
    "                self.simulator,\n",
    "                self.system,\n",
    "                self.nominal_controller,\n",
    "                self.controller,\n",
    "                self.datafiles,\n",
    "                self.logger,\n",
    "                self.actor_optimizer,\n",
    "                self.critic_optimizer,\n",
    "                self.running_objective,\n",
    "                self.scenario,\n",
    "            ),\n",
    "            pars=(\n",
    "                self.state_init,\n",
    "                self.action_init,\n",
    "                self.time_start,\n",
    "                self.time_final,\n",
    "                self.state_init,\n",
    "                self.xMin,\n",
    "                self.xMax,\n",
    "                self.yMin,\n",
    "                self.yMax,\n",
    "                self.control_mode,\n",
    "                self.action_manual,\n",
    "                self.v_min,\n",
    "                self.omega_min,\n",
    "                self.v_max,\n",
    "                self.omega_max,\n",
    "                self.Nruns,\n",
    "                self.no_print,\n",
    "                self.is_log,\n",
    "                0,\n",
    "                [],\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "    def execute_pipeline(self, **kwargs):\n",
    "        self.load_config()\n",
    "        self.setup_env()\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.initialize_system()\n",
    "        self.initialize_predictor()\n",
    "        self.initialize_safe_controller()\n",
    "        self.initialize_models()\n",
    "        self.initialize_objectives()\n",
    "        self.initialize_optimizers()\n",
    "        self.initialize_actor_critic()\n",
    "        self.initialize_controller()\n",
    "        self.initialize_simulator()\n",
    "        self.initialize_logger()\n",
    "        self.initialize_scenario()\n",
    "        if not self.no_visual and not self.save_trajectory:\n",
    "            self.initialize_visualizer()\n",
    "            self.main_loop_visual()\n",
    "        else:\n",
    "            self.scenario.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline3WRobotNISeminar().execute_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120c5507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline3WRobotNISeminarCasadi(Pipeline3WRobotNI):\n",
    "    def initialize_optimizers(self):\n",
    "\n",
    "        opt_options = {\n",
    "            \"print_time\": 0,\n",
    "            \"ipopt.max_iter\": 200,\n",
    "            \"ipopt.print_level\": 0,\n",
    "            \"ipopt.acceptable_tol\": 1e-7,\n",
    "            \"ipopt.acceptable_obj_change_tol\": 1e-2,\n",
    "        }\n",
    "\n",
    "        self.actor_optimizer = optimizers.CasADiOptimizer(\n",
    "            opt_method=\"ipopt\", opt_options=opt_options\n",
    "        )\n",
    "        self.critic_optimizer = optimizers.CasADiOptimizer(\n",
    "            opt_method=\"ipopt\", opt_options=opt_options,\n",
    "        )\n",
    "        \n",
    "Pipeline3WRobotNISeminarCasadi().execute_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4280a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
